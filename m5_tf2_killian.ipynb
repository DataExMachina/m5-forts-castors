{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network regression using tensorflow\n",
    "author : Killian Pitiot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "from multiprocessing import Pool  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfkl = tfk.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# custom imports\n",
    "warnings.filterwarnings('ignore')  # to avoid tfp bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL_DTYPES = {\"event_name_1\": \"category\",\n",
    "              \"event_name_2\": \"category\",\n",
    "              \"event_type_1\": \"category\",\n",
    "              \"event_type_2\": \"category\",\n",
    "              \"weekday\": \"category\",\n",
    "              'wm_yr_wk': 'int16',\n",
    "              \"wday\": \"int16\",\n",
    "              \"month\": \"int16\",\n",
    "              \"year\": \"int16\",\n",
    "              \"snap_CA\": \"float32\",\n",
    "              'snap_TX': 'float32',\n",
    "              'snap_WI': 'float32'}\n",
    "\n",
    "PRICE_DTYPES = {\"store_id\": \"category\",\n",
    "                \"item_id\": \"category\",\n",
    "                \"wm_yr_wk\": \"int16\",\n",
    "                \"sell_price\": \"float32\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data  \n",
    "\n",
    "sales_train_validation = pd.read_csv('./data/raw/sales_train_validation.csv')\n",
    "increasing_term = sales_train_validation.groupby(['dept_id', 'store_id'])\\\n",
    "                [['d_%s' % c for c in range(1,1914)]].sum()\n",
    "\n",
    "increasing_term = (increasing_term.T - increasing_term.T.shift(28))/increasing_term.T.shift(28)\n",
    "increasing_term = increasing_term.reset_index(drop=True).iloc[-365:,:]\n",
    "\n",
    "rates = increasing_term[increasing_term.abs()<1].mean()+1\n",
    "rates = rates.reset_index().rename(columns={0:'rate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 4, 25, 0, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training constants \n",
    "use_sampled_ds = False # does not work yet  \n",
    "h = 28 \n",
    "max_lags = 366\n",
    "tr_last = 1913\n",
    "fday = datetime(2016,4, 25) \n",
    "fday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dt(is_train=True, nrows=None, first_day=1200):\n",
    "    prices = pd.read_csv(\"./data/raw/sell_prices.csv\", dtype=PRICE_DTYPES)\n",
    "    for col, col_dtype in PRICE_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            prices[col] = prices[col].cat.codes.astype(\"int16\")\n",
    "            prices[col] -= prices[col].min()\n",
    "\n",
    "    cal = pd.read_csv(\"./data/raw/calendar.csv\", dtype=CAL_DTYPES)\n",
    "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
    "    for col, col_dtype in CAL_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            cal[col] = cal[col].cat.codes.astype(\"int16\")\n",
    "            cal[col] -= cal[col].min()\n",
    "\n",
    "    start_day = max(1 if is_train else tr_last-max_lags, first_day)\n",
    "    numcols = [f\"d_{day}\" for day in range(start_day, tr_last+1)]\n",
    "    catcols = ['id', 'item_id', 'dept_id', 'store_id', 'cat_id', 'state_id']\n",
    "    dtype = {numcol: \"float32\" for numcol in numcols}\n",
    "    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n",
    "    dt = pd.read_csv(\"./data/raw/sales_train_validation.csv\",\n",
    "                     nrows=nrows, usecols=catcols + numcols, dtype=dtype)\n",
    "\n",
    "    for col in catcols:\n",
    "        if col != \"id\":\n",
    "            dt[col] = dt[col].cat.codes.astype(\"int16\")\n",
    "            dt[col] -= dt[col].min()\n",
    "\n",
    "    increasing_term = dt.groupby(['dept_id', 'store_id'])[numcols].sum()\n",
    "    increasing_term = (increasing_term.T -\n",
    "                       increasing_term.T.shift(28))/increasing_term.T.shift(28)\n",
    "    increasing_term = increasing_term.reset_index(drop=True).iloc[-365:, :]\n",
    "    rates = increasing_term[increasing_term.abs() < 1].mean()+1\n",
    "    rates = rates.reset_index().rename(columns={0: 'rate'})\n",
    "\n",
    "    if not is_train:\n",
    "        for day in range(tr_last+1, tr_last + 2*h + 1):\n",
    "            dt[f\"d_{day}\"] = np.nan\n",
    "\n",
    "    dt = pd.melt(dt,\n",
    "                 id_vars=catcols,\n",
    "                 value_vars=[\n",
    "                     col for col in dt.columns if col.startswith(\"d_\")],\n",
    "                 var_name=\"d\",\n",
    "                 value_name=\"sales\")\n",
    "\n",
    "    dt = dt.merge(cal, on=\"d\", copy=False)\n",
    "    dt = dt.merge(prices, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], copy=False)\n",
    "    dt = dt.merge(rates, how='left')\n",
    "\n",
    "    return dt\n",
    "\n",
    "\n",
    "def create_features(dt):\n",
    "    lags = [7, 28]\n",
    "    lag_cols = [f\"lag_{lag}\" for lag in lags]\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        dt[lag_col] = dt[[\"id\", \"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
    "\n",
    "    wins = [7, 28]\n",
    "    for win in wins:\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\n",
    "                \"id\")[lag_col].transform(lambda x: x.rolling(win).mean())\n",
    "\n",
    "    date_features = {\n",
    "\n",
    "        \"wday\": \"weekday\",\n",
    "        \"week\": \"weekofyear\",\n",
    "        \"month\": \"month\",\n",
    "        \"quarter\": \"quarter\",\n",
    "        \"year\": \"year\",\n",
    "        \"mday\": \"day\",\n",
    "    }\n",
    "\n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        if date_feat_name in dt.columns:\n",
    "            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n",
    "        else:\n",
    "            dt[date_feat_name] = getattr(\n",
    "                dt[\"date\"].dt, date_feat_func).astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_DAY = 1000 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tfk.callbacks.Callback):\n",
    "    def __init__(self, print_n_epochs=10):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.print_n_epochs = print_n_epochs\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch%self.print_n_epochs==0:\n",
    "            try:\n",
    "                print(f\"Epoch {epoch} loss : {logs['loss']:.2f}, val_rmse :{logs['val_root_mean_squared_error']:.2f}\")\n",
    "            except:\n",
    "                print(f\"Epoch {epoch} loss : {logs['loss']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = create_dt(is_train=True, first_day= FIRST_DAY)\n",
    "create_features(df)\n",
    "df.dropna(inplace = True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weights for the training (the older the sample the less it will have impact )\n",
    "weights = df['d'].str[2:].astype(int)\n",
    "weights = weights/np.sum(weights)\n",
    "\n",
    "# weights based on the amplitude of the sales\n",
    "p_sampling = df['sales']/df['sales'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = ['item_id',\n",
    "             'dept_id',\n",
    "             'store_id',\n",
    "             'cat_id',\n",
    "             'state_id'] + \\\n",
    "            [\"event_name_1\",\n",
    "             \"event_name_2\",\n",
    "             \"event_type_1\",\n",
    "             \"event_type_2\"]\n",
    "\n",
    "useless_cols = [\"id\",\n",
    "                \"date\",\n",
    "                \"sales\",\n",
    "                \"d\",\n",
    "                \"wm_yr_wk\",\n",
    "                \"weekday\"\n",
    "               ]\n",
    "\n",
    "\n",
    "num_feats = df.columns[~df.columns.isin(useless_cols+cat_feats)].to_list()\n",
    "train_cols = num_feats+cat_feats\n",
    "\n",
    "X_train = df[train_cols]\n",
    "y_train = df[\"sales\"]\n",
    "\n",
    "np.random.seed(777)\n",
    "\n",
    "fake_valid_inds = np.random.choice(\n",
    "    X_train.index.values, 2_000_000, replace=False)\n",
    "train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n",
    "\n",
    "X_test, y_test = X_train.loc[fake_valid_inds], y_train.loc[fake_valid_inds]\n",
    "X_train, y_train = X_train.loc[train_inds], y_train.loc[train_inds]\n",
    "\n",
    "X_train[num_feats], X_test[num_feats] = X_train[num_feats].astype(\n",
    "    np.float32), X_test[num_feats].astype(np.float32)\n",
    "\n",
    "X_train[cat_feats], X_test[cat_feats] = X_train[cat_feats].astype(\n",
    "    np.int32), X_test[cat_feats].astype(np.int32)\n",
    "\n",
    "cardinality = df[cat_feats].nunique()\n",
    "weights_train = weights.loc[X_train.index]\n",
    "p_sampling_train = p_sampling.loc[X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare input for tensorflow\n",
    "# as we have multiple input type the best solution is to feed a dict like object\n",
    "\n",
    "input_dict = {f\"input_{col}\": X_train[col] for col in X_train.columns}\n",
    "\n",
    "input_dict_test = {f\"input_{col}\": X_test[col] for col in X_train.columns}\n",
    "\n",
    "del df, X_train, X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of target unique values \n",
    "if use_sampled_ds:\n",
    "    tf_ds = tf.data.Dataset.from_tensor_slices((input_dict,{'target' : y_train.values}))\n",
    "    tf_ds = tf_ds.shuffle(5000).repeat()\n",
    "    ## list for all different sales (out of memory)\n",
    "    target_unique =np.unique(y_train.values)\n",
    "    weigth_sampling = [1/len(target_unique) for target_ in target_unique]\n",
    "    list_tf_ds = [(tf_ds.filter(lambda features, target: target==target_value))\\\n",
    "             for target_value in target_unique]\n",
    "    \n",
    "    ##  make stratified sampling \n",
    "    ## list for all different sales (out of memory)\n",
    "#     tf1 = tf_ds.filter(lambda features, target: target==target_value)\n",
    "#     weigth_sampling = [target_/np.sum(target_unique) for target_ in target_unique]\n",
    "#     list_tf_ds = [(tf_ds.filter(lambda features, target: target==target_value)).repeat()\\\n",
    "#              for target_value in target_unique]\n",
    "    \n",
    "    tf_ds = tf.data.experimental.sample_from_datasets(list_tf_ds,weigth_sampling)\n",
    "    tf_ds = tf_ds.take(2).batch(8,drop_remainder=True).prefetch(2)\n",
    "#     del input_dict \n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter_fn(n):\n",
    "    def filter_fn(x, y):\n",
    "        return tf.equal(1.0, y[n])\n",
    "    return filter_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset(name, dataset):\n",
    "    elems = np.array([v.numpy() for v in dataset])\n",
    "    print(\"Dataset {} contains {} elements :\".format(name, len(elems)))\n",
    "    print(elems)\n",
    "    \n",
    "print_dataset(\"ih\", tf_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the tensorflow modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=C:\\Users\\Killian\\Documents\\python_code\\Kaggle\\m5-forecasting-accuracy\\logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_item_id (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_dept_id (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_store_id (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_cat_id (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_state_id (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_event_name_1 (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_event_name_2 (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_event_type_1 (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_event_type_2 (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_item_id (Embedding)   (None, 1, 30)        91500       input_item_id[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_dept_id (Embedding)   (None, 1, 4)         32          input_dept_id[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_store_id (Embedding)  (None, 1, 5)         55          input_store_id[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_cat_id (Embedding)    (None, 1, 2)         8           input_cat_id[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_state_id (Embedding)  (None, 1, 2)         8           input_state_id[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_event_name_1 (Embeddi (None, 1, 16)        512         input_event_name_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_event_name_2 (Embeddi (None, 1, 2)         8           input_event_name_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_event_type_1 (Embeddi (None, 1, 3)         18          input_event_type_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_event_type_2 (Embeddi (None, 1, 2)         8           input_event_type_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_wday (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_month (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_year (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_snap_CA (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_snap_TX (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_snap_WI (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_sell_price (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_rate (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_lag_7 (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_lag_28 (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_rmean_7_7 (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_rmean_28_7 (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_rmean_7_28 (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_rmean_28_28 (InputLayer)  [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_week (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_quarter (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mday (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1, 30)        0           embedding_item_id[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 4)         0           embedding_dept_id[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1, 5)         0           embedding_store_id[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1, 2)         0           embedding_cat_id[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1, 2)         0           embedding_state_id[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1, 16)        0           embedding_event_name_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1, 2)         0           embedding_event_name_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1, 3)         0           embedding_event_type_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1, 2)         0           embedding_event_type_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_num (Concatenate)   (None, 17)           0           input_wday[0][0]                 \n",
      "                                                                 input_month[0][0]                \n",
      "                                                                 input_year[0][0]                 \n",
      "                                                                 input_snap_CA[0][0]              \n",
      "                                                                 input_snap_TX[0][0]              \n",
      "                                                                 input_snap_WI[0][0]              \n",
      "                                                                 input_sell_price[0][0]           \n",
      "                                                                 input_rate[0][0]                 \n",
      "                                                                 input_lag_7[0][0]                \n",
      "                                                                 input_lag_28[0][0]               \n",
      "                                                                 input_rmean_7_7[0][0]            \n",
      "                                                                 input_rmean_28_7[0][0]           \n",
      "                                                                 input_rmean_7_28[0][0]           \n",
      "                                                                 input_rmean_28_28[0][0]          \n",
      "                                                                 input_week[0][0]                 \n",
      "                                                                 input_quarter[0][0]              \n",
      "                                                                 input_mday[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_item_id (Flatten)       (None, 30)           0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_dept_id (Flatten)       (None, 4)            0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_store_id (Flatten)      (None, 5)            0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_cat_id (Flatten)        (None, 2)            0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_state_id (Flatten)      (None, 2)            0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_event_name_1 (Flatten)  (None, 16)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_event_name_2 (Flatten)  (None, 2)            0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_event_type_1 (Flatten)  (None, 3)            0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_event_type_2 (Flatten)  (None, 2)            0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 17)           68          concatenate_num[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_cat (Concatenate)   (None, 66)           0           flatten_item_id[0][0]            \n",
      "                                                                 flatten_dept_id[0][0]            \n",
      "                                                                 flatten_store_id[0][0]           \n",
      "                                                                 flatten_cat_id[0][0]             \n",
      "                                                                 flatten_state_id[0][0]           \n",
      "                                                                 flatten_event_name_1[0][0]       \n",
      "                                                                 flatten_event_name_2[0][0]       \n",
      "                                                                 flatten_event_type_1[0][0]       \n",
      "                                                                 flatten_event_type_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_all (Concatenate)   (None, 83)           0           batch_normalization[0][0]        \n",
      "                                                                 concatenate_cat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Dense_0 (Dense)                 (None, 512)          43008       concatenate_all[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           Dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512)          2048        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dense_1 (Dense)                 (None, 256)          131328      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256)          0           Dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256)          1024        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dense_2 (Dense)                 (None, 128)          32896       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 128)          0           Dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128)          512         dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dense_3 (Dense)                 (None, 64)           8256        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 64)           0           Dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64)           256         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, 1)            65          batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 311,610\n",
      "Trainable params: 309,656\n",
      "Non-trainable params: 1,954\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TF2 model\n",
    "# loss for a poisson regression\n",
    "\n",
    "\n",
    "def poisson(y_true, y_pred):\n",
    "    '''\n",
    "    Loss computed as a Poisson regression \n",
    "    '''\n",
    "    return K.mean(K.maximum(.0, y_pred) - y_true * K.log(K.maximum(.0, y_pred) + K.epsilon()), axis=-1)\n",
    "\n",
    "\n",
    "def tweedie_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Tweedie regression, same style as poisson but ... well ... different \n",
    "\n",
    "    '''\n",
    "    p = 1.5\n",
    "    dev = K.pow(y_true, 2-p)/((1-p) * (2-p)) \\\n",
    "        - y_true * K.pow(K.maximum(.0, y_pred) + K.epsilon(), 1-p)/(1-p) \\\n",
    "        + K.pow(K.maximum(.0, y_pred) + K.epsilon(), 2-p)/(2-p)\n",
    "\n",
    "    return K.mean(dev, axis=-1)\n",
    "\n",
    "\n",
    "alpha = .5\n",
    "\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    '''\n",
    "    make a comprised loss of poisson and tweedie distribution\n",
    "    '''\n",
    "    return (1 - alpha) * poisson(y_true, y_pred) + alpha * tweedie(y_true, y_pred)\n",
    "\n",
    "\n",
    "# function to generate the MLP\n",
    "def create_mlp(layers_list=[512, 256, 128, 64],\n",
    "               emb_dim=30,\n",
    "               loss_fn='poisson',\n",
    "               learning_rate=1e-3,\n",
    "               optimizer=tfk.optimizers.Adam,\n",
    "               verbose=0):\n",
    "    '''\n",
    "    description : \n",
    "    generate regression mlp with\n",
    "    both embedding entries for categorical features and \n",
    "    standard inputs for numerical features\n",
    "\n",
    "    params:\n",
    "    layers_list : list of layers dimensions \n",
    "    emb_dim : maximum embedding size\n",
    "    output :\n",
    "    uncompiled keras model  \n",
    "    '''\n",
    "\n",
    "    # define our MLP network\n",
    "    layers = []\n",
    "    output_num = []\n",
    "    inputs = []\n",
    "    output_cat = []\n",
    "    output_num = []\n",
    "\n",
    "    # sequencial inputs\n",
    "\n",
    "    # numerical data part\n",
    "    if len(num_feats) > 1:\n",
    "        for num_var in num_feats:\n",
    "            input_num = tfkl.Input(\n",
    "                shape=(1,), name='input_{0}'.format(num_var))\n",
    "            inputs.append(input_num)\n",
    "            output_num.append(input_num)\n",
    "        output_num = tfkl.Concatenate(name='concatenate_num')(output_num)\n",
    "        output_num = tfkl.BatchNormalization()(output_num) # to avoid preprocessing \n",
    "\n",
    "    else:\n",
    "        input_num = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(numeric_features[0]))\n",
    "        inputs.append(input_num)\n",
    "        output_num = input_num\n",
    "\n",
    "    # categorical data input\n",
    "    for categorical_var in cat_feats:\n",
    "        no_of_unique_cat = cardinality[categorical_var]\n",
    "        if verbose == 1:\n",
    "            print(fcategorical_var, no_of_unique_cat)\n",
    "        embedding_size = min(np.ceil((no_of_unique_cat)/2), emb_dim)\n",
    "        embedding_size = int(embedding_size)\n",
    "        vocab = no_of_unique_cat+1\n",
    "        \n",
    "        # functionnal loop\n",
    "        input_cat = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(categorical_var))\n",
    "        inputs.append(input_cat)\n",
    "        embedding = tfkl.Embedding(vocab,\n",
    "                                   embedding_size,\n",
    "                                   embeddings_regularizer = tf.keras.regularizers.l1(1e-8),\n",
    "                                   name='embedding_{0}'.format(categorical_var))(input_cat)\n",
    "        embedding = tfkl.Dropout(0.15)(embedding)\n",
    "        vec = tfkl.Flatten(name='flatten_{0}'.format(\n",
    "            categorical_var))(embedding)\n",
    "        output_cat.append(vec)\n",
    "    output_cat = tfkl.Concatenate(name='concatenate_cat')(output_cat)\n",
    "    \n",
    "    # concatenate numerical input and embedding output\n",
    "    dense = tfkl.Concatenate(name='concatenate_all')([output_num, output_cat])\n",
    "\n",
    "    # dense network \n",
    "    for i in range(len(layers_list)):\n",
    "        dense = tfkl.Dense(layers_list[i],\n",
    "                           name='Dense_{0}'.format(str(i)),\n",
    "                           activation='elu')(dense)\n",
    "        dense = tfkl.Dropout(.15)(dense)\n",
    "        dense = tfkl.BatchNormalization()(dense)\n",
    "\n",
    "    dense2 = tfkl.Dense(1, name='Output', activation='elu')(dense)\n",
    "    model = tfk.Model(inputs, dense2)\n",
    "\n",
    "    \n",
    "    opt = optimizer(learning_rate)\n",
    "    \n",
    "    # choose the type of regression \n",
    "    if loss_fn == 'poisson':\n",
    "        model.compile(loss=poisson, optimizer=opt, metrics=[\n",
    "                      tf.keras.metrics.RootMeanSquaredError()])\n",
    "    elif loss_fn == 'tweedie':\n",
    "        model.compile(loss=tweedie_loss, optimizer=opt, metrics=[\n",
    "                      tf.keras.metrics.RootMeanSquaredError()])\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Loss function should be either Poisson or tweedie for now\")\n",
    "    return model\n",
    "\n",
    "# remove the computing graph if exist (maybe no more required if TF2)\n",
    "try:\n",
    "    del mdl\n",
    "except:\n",
    "    pass\n",
    "\n",
    "mdl = create_mlp(loss_fn='tweedie')\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:From D:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_root_mean_squared_error` which is not available. Available metrics are: \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    496\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 497\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[1;34m(input_iterator)\u001b[0m\n\u001b[0;32m     84\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[1;32m---> 85\u001b[1;33m         per_replica_function, args=args)\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m    762\u001b[0m                                 convert_by_default=False)\n\u001b[1;32m--> 763\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1818\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1819\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2163\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[1;32m-> 2164\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics, standalone)\u001b[0m\n\u001b[0;32m    432\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    311\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    313\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m           \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m           if isinstance(model.optimizer,\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1221\u001b[0m       grad.dtype in (dtypes.int32, dtypes.float32)):\n\u001b[1;32m-> 1222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1223\u001b[0m   \u001b[1;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" vs. \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6124\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[1;32m-> 6125\u001b[1;33m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[0;32m   6126\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    741\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3321\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3322\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3323\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1785\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1786\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1787\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1604\u001b[1;33m       \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_AddInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f6e22b98ee08>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch} loss : {logs['loss']:.2f}, val_rmse :{logs['val_root_mean_squared_error']:.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'loss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-34e49be8ff31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m                       \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# weighting the grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                       \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCustomCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                       \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                       )\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\anaconda\\envs\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-f6e22b98ee08>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch} loss : {logs['loss']:.2f}, val_rmse :{logs['val_root_mean_squared_error']:.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch} loss : {logs['loss']:.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "# training of the algorithm \n",
    "\n",
    "# checkpointsthe model to reload the best parameters\n",
    "model_save = tfk.callbacks.ModelCheckpoint('model_checkpoints')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tfk.callbacks.EarlyStopping('val_root_mean_squared_error',\n",
    "                                             patience=10,\n",
    "                                             verbose=0,\n",
    "                                             restore_best_weights=True)\n",
    "\n",
    "# Logging for the tensorboard following\n",
    "log_dir = \"logs\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# # Tensorbard callback\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n",
    "#                                                       histogram_freq=1)\n",
    "\n",
    "\n",
    "\n",
    "## fit tf dataset and batch sampling  \n",
    "if use_sampled_ds:\n",
    "    # training with a custom loop \n",
    "    history = mdl.fit(tf_ds,\n",
    "                      #validation_data=(input_dict_test, y_test.values),\n",
    "                      epochs=10,\n",
    "#                       shuffle=True,\n",
    "                      #steps_per_epoch = 1000, # as we are using sampling \n",
    "#                       callbacks=[CustomCallback(1)],\n",
    "                      verbose=0\n",
    "                      )\n",
    "else:\n",
    "    # training with the weighted input dict \n",
    "    history = mdl.fit(input_dict,\n",
    "                      y_train.values,\n",
    "                      validation_data=(input_dict_test, y_test.values),\n",
    "                      batch_size=4096,\n",
    "                      epochs=100,\n",
    "                      shuffle=True,\n",
    "                      sample_weight=weights_train.values, # weighting the grads\n",
    "                      callbacks=[model_save, early_stopping,CustomCallback(10)],\n",
    "                      verbose=0,\n",
    "                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.save('keras_tweedie.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['root_mean_squared_error'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model rmse')\n",
    "plt.ylabel('rmse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig('poisson.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make optimization for the NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports we know we'll need only for BGS \n",
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.callbacks import CheckpointSaver, VerboseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start to generate the search space for the gaussian process\n",
    "\n",
    "dim_learning_rate = Real(low=1e-3, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_num_dense_layers = Integer(low=1, high=3, name='num_dense_layers')\n",
    "dim_num_dense_nodes = Integer(low=32, high=2048, name='num_dense_nodes')\n",
    "\n",
    "dim_batch_size = Integer(low=2048, high=10000, name='batch_size')\n",
    "dim_emb_dim = Integer(low=10, high=50, name='emb_dim')\n",
    "\n",
    "dim_loss_fn = Categorical(categories=['poisson', 'tweedie'], name='loss_fn')\n",
    "\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_emb_dim,\n",
    "              dim_batch_size,\n",
    "              dim_loss_fn,\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the optimization of HP functions using skopt\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate,\n",
    "            num_dense_layers,\n",
    "            num_dense_nodes,\n",
    "            batch_size,\n",
    "            emb_dim,\n",
    "            loss_fn,\n",
    "            ):\n",
    "\n",
    "    # generate a list of the MLP architecture\n",
    "    # Enforce a funnel like structure\n",
    "    list_layer = [num_dense_nodes//(2**x) for x in range(num_dense_layers)]\n",
    "\n",
    "    model = create_mlp(layers_list=list_layer,\n",
    "                       emb_dim=emb_dim,\n",
    "                       loss_fn='poisson',\n",
    "                       learning_rate=learning_rate,\n",
    "                       optimizer=tfk.optimizers.Adam\n",
    "                       )\n",
    "\n",
    "    print(\n",
    "        f'Generated a model with {model.count_params()} trainable parameters')\n",
    "\n",
    "    # checkpointsthe model to reload the best parameters\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree('./model_checkpoints')\n",
    "        print('Old checkpoint remove')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    model_save = tfk.callbacks.ModelCheckpoint('model_checkpoints')\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = tfk.callbacks.EarlyStopping('val_root_mean_squared_error',\n",
    "                                                 patience=10,\n",
    "                                                 verbose = 0,\n",
    "                                                 restore_best_weights=True)\n",
    "    history = model.fit(input_dict,\n",
    "                        y_train.values,\n",
    "                        validation_data=(input_dict_test, y_test.values),\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=100,\n",
    "                        shuffle=True,\n",
    "                        # importance of the samples based of the ages\n",
    "                        sample_weight=weights_train.values,\n",
    "                        callbacks=[model_save, early_stopping,,CustomCallback(10)],\n",
    "                        verbose=0,\n",
    "                        )\n",
    "    # return the validation accuracy for the last epoch.\n",
    "    rmse = history.history['val_root_mean_squared_error'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"RMSE: {:.2}\".format(rmse))\n",
    "    print()\n",
    "\n",
    "    global best_rmse\n",
    "    if rmse < best_rmse:\n",
    "        model.save('keras_best_model.h5')\n",
    "        best_rmse = rmse\n",
    "    print(\"Best RMSE: {:.2}\".format(rmse))\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory(not sure if needed in tf2)\n",
    "    del model, early_stopping, model_save\n",
    "    # Clear the Keras session\n",
    "    K.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    gc.collect()\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the optimization interuptible\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "checkpoint_callback = skopt.callbacks.CheckpointSaver(\"./result.pkl\")\n",
    "\n",
    "best_rmse = 1000\n",
    "gp_result = gp_minimize(func=fitness,\n",
    "                        dimensions=dimensions,\n",
    "                        n_calls=20,\n",
    "                        n_jobs=1,\n",
    "                        verbose=True,\n",
    "                        callback=[checkpoint_callback],\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gp_result.best_estimator_)\n",
    "print(gp_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_objective\n",
    "_ = plot_convergence(gp_result)\n",
    "_ = plot_objective(gp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the forecast results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "alphas = [1.035, 1.03, 1.025]\n",
    "weights = [1/len(alphas)]*len(alphas)\n",
    "sub = 0.\n",
    "\n",
    "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
    "\n",
    "    te = create_dt(False)\n",
    "    cols = [f\"F{i}\" for i in range(1, 29)]\n",
    "\n",
    "    for tdelta in range(0, 28):\n",
    "        day = fday + timedelta(days=tdelta)\n",
    "        print(icount, day)\n",
    "        tst = te[(te.date >= day - timedelta(days=max_lags))\n",
    "                 & (te.date <= day)].copy()\n",
    "        create_fea(tst)\n",
    "        tst = tst.loc[tst.date == day, train_cols]\n",
    "        input_dict_predict = {f\"input_{col}\": tst[col] for col in tst.columns}\n",
    "        pred = mdl.predict(input_dict_predict, batch_size=10000)\n",
    "        te.loc[te.date == day, \"sales\"] = alpha*pred\n",
    "        print(pred)\n",
    "\n",
    "    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
    "#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h),\n",
    "#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\n",
    "        \"id\"].cumcount()+1]\n",
    "    te_sub = te_sub.set_index([\"id\", \"F\"]).unstack()[\n",
    "        \"sales\"][cols].reset_index()\n",
    "    te_sub.fillna(0., inplace=True)\n",
    "    te_sub.sort_values(\"id\", inplace=True)\n",
    "    te_sub.reset_index(drop=True, inplace=True)\n",
    "    te_sub.to_csv(f\"submission_{icount}.csv\", index=False)\n",
    "    if icount == 0:\n",
    "        sub = te_sub\n",
    "        sub[cols] *= weight\n",
    "    else:\n",
    "        sub[cols] += te_sub[cols]*weight\n",
    "    print(icount, alpha, weight)\n",
    "\n",
    "\n",
    "sub2 = sub.copy()\n",
    "sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFP  : modify training and loss to adapt it to bayesian training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "\n",
    "# prior distribution on the weights \n",
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t, scale=1),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "# layer posterior distribution with mean field approximation  \n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1.))\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t[..., :n],\n",
    "                       scale=1e-5 + 0.02*tf.nn.softplus(c + t[..., n:])),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "# probabilistic  loss functions\n",
    "def neg_log_likelihood_continuous(y_true, y_pred):\n",
    "    '''\n",
    "    negative log likelyhood for stricly positive distributions \n",
    "    \n",
    "    '''\n",
    "    return -y_pred.prob(y_true+1e-6)\n",
    "\n",
    "\n",
    "def neg_log_likelihood_discrete(y_true, y_pred):\n",
    "    '''\n",
    "    negloglik when we don't care about the probability in zero \n",
    "    '''\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "# custom metrics\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    '''\n",
    "    compact implemtation of the rmse \n",
    "    '''\n",
    "    return tf.math.sqrt(tf.math.reduce_mean(tf.math.square(y_pred-y_true)))\n",
    "\n",
    "\n",
    "# to much scaling: high variance, no enough -> converge to the likelyhood\n",
    "kl_weight = batch_size/training_size \n",
    "kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda\n",
    "                          kl_weight)  # KL over the batch of 2048 (should normalize by the number of mini batch)\n",
    "\n",
    "\n",
    "def create_mlp(layers_list=[512, 512, 512, 64],\n",
    "               type_output='poisson'):\n",
    "    '''\n",
    "    description : \n",
    "    generate regression mlp with\n",
    "    both embedding entries for categorical features and \n",
    "    standard inputs for numerical features\n",
    "\n",
    "    params:\n",
    "    layers_list : list of layers dimensions \n",
    "    output :\n",
    "    compiled keras model  \n",
    "    '''\n",
    "\n",
    "    # define our MLP network\n",
    "    layers = []\n",
    "    output_num = []\n",
    "    inputs = []\n",
    "    output_cat = []\n",
    "    output_num = []\n",
    "\n",
    "    # numerical data part\n",
    "    if len(num_feats) > 1:\n",
    "        for num_var in num_feats:\n",
    "            input_num = tfkl.Input(\n",
    "                shape=(1,), name='input_{0}'.format(num_var))\n",
    "            inputs.append(input_num)\n",
    "            output_num.append(input_num)\n",
    "        output_num = tfkl.Concatenate(name='concatenate_num')(output_num)\n",
    "        #output_num = tfkl.BatchNormalization()(output_num)\n",
    "\n",
    "    else:\n",
    "        input_num = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(numeric_features[0]))\n",
    "        inputs.append(input_num)\n",
    "        output_num = input_num\n",
    "\n",
    "    # create an embedding for every categorical feature\n",
    "    for categorical_var in cat_feats:\n",
    "        # should me nunique() but events are poorly preprocessed\n",
    "        no_of_unique_cat = cardinality[categorical_var]\n",
    "        print(categorical_var, no_of_unique_cat)\n",
    "        embedding_size = min(np.ceil((no_of_unique_cat)/2), 30)\n",
    "        embedding_size = int(embedding_size)\n",
    "        vocab = no_of_unique_cat+1\n",
    "        # functionnal loop\n",
    "        input_cat = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(categorical_var))\n",
    "        inputs.append(input_cat)\n",
    "        embedding = tfkl.Embedding(vocab,\n",
    "                                   embedding_size,\n",
    "                                   embeddings_regularizer = tf.keras.regularizers.l1(1e-8),\n",
    "                                   name='embedding_{0}'.format(categorical_var))(input_cat)\n",
    "        embedding = tfkl.Dropout(0.1)(embedding)\n",
    "        vec = tfkl.Flatten(name='flatten_{0}'.format(\n",
    "            categorical_var))(embedding)\n",
    "        output_cat.append(vec)\n",
    "    output_cat = tfkl.Concatenate(name='concatenate_cat')(output_cat)\n",
    "\n",
    "    # concatenate numerical input and embedding output\n",
    "    dense = tfkl.Concatenate(name='concatenate_all')([output_num, output_cat])\n",
    "\n",
    "    for i in range(len(layers_list)):\n",
    "        dense = tfkl.Dense(layers_list[i],\n",
    "                           name='Dense_{0}'.format(str(i)),\n",
    "                           activation='elu')(dense)\n",
    "        dense = tfkl.Dropout(.1)(dense)\n",
    "        dense = tfkl.BatchNormalization()(dense)\n",
    "\n",
    "    # lognormal\n",
    "    if type_output == 'gaussian':\n",
    "        dense2 = tfk.layers.Dense(2,\n",
    "                                  activation='softplus',\n",
    "                                  name='Output'\n",
    "                                  )(dense)\n",
    "\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Normal(t[..., 0], scale=t[..., 1:]))(dense2)\n",
    "\n",
    "    # Poisson\n",
    "    elif type_output == 'poisson':\n",
    "        dense2 = tfk.layers.Dense(1,\n",
    "                                  name='Output')(dense)\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Poisson(rate=tf.math.softplus(t[..., 0])))(dense2)\n",
    "\n",
    "    # Gamma\n",
    "    elif type_output == 'gamma':\n",
    "        dense2 = tfk.layers.Dense(2,\n",
    "                                  name='Output',\n",
    "                                  activation='softplus',\n",
    "                                  )(dense)\n",
    "\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Gamma(concentration=.01*t[..., 0],\n",
    "                                rate=.01*t[..., 1]))(dense2)\n",
    "    else:\n",
    "        output = tfk.Dense(1)\n",
    "\n",
    "    model = tfk.Model(inputs, output)\n",
    "    opt = tfk.optimizers.Nadam(learning_rate=1e-2)\n",
    "\n",
    "    model.compile(loss=neg_log_likelihood_continuous,\n",
    "                  optimizer=opt,\n",
    "                  metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "\n",
    "try:\n",
    "    del mdl_tfp\n",
    "    print('model deleted ')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "mdl_tfp = create_mlp(type_output='poisson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "history = mdl_tfp.fit(input_dict,\n",
    "                      y_train.values,\n",
    "                      validation_data=(input_dict_test, y_test.values),\n",
    "                      batch_size=4096,\n",
    "                      epochs=2,\n",
    "                      shuffle=True,\n",
    "                      verbose=1,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make  a custom batch generator \n",
    "based on https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tfk.utils.Sequence):\n",
    "    'Generates data for tfk'\n",
    "\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32, 32, 32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # X : (n_samples, *dim, n_channels)\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i, ] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a custom random sampled training loop\n",
    "# # need to call the model with training = True for the custom training to handle dropout and BN\n",
    "\n",
    "# def random_batch(input_X,y,batch_size=128,sample_weights=None):\n",
    "#     # take a sample batch based on the weighting\n",
    "#     idx = np.random.choice(range(len(input_X)),sample_weights,batch_size)\n",
    "#     return input_X[idx],y[idx]\n",
    "\n",
    "\n",
    "# model = create_mlp()\n",
    "\n",
    "# # initialoze parameters\n",
    "# n_epochs = 5\n",
    "# batch_size = 2048\n",
    "# n_steps = 50\n",
    "# opt = tfk.optimizers.Adam(learning_rate=1e-3)\n",
    "# loss_fn = poisson\n",
    "# mean_loss = tfk.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fully bayesian network\n",
    "\n",
    "def create_mlp_full_bayes(layers_list=[512, 256, 128, 64],\n",
    "                          type_output='poisson'):\n",
    "    '''\n",
    "    description : \n",
    "    generate regression mlp with\n",
    "    both embedding entries for categorical features and \n",
    "    standard inputs for numerical features\n",
    "\n",
    "    params:\n",
    "    layers_list : list of layers dimensions \n",
    "    output :\n",
    "    compiled keras model  \n",
    "    '''\n",
    "\n",
    "    # define our MLP network\n",
    "    layers = []\n",
    "    output_num = []\n",
    "    inputs = []\n",
    "    output_cat = []\n",
    "    output_num = []\n",
    "\n",
    "    # numerical data part\n",
    "    if len(num_feats) > 1:\n",
    "        for num_var in num_feats:\n",
    "            input_num = tfkl.Input(\n",
    "                shape=(1,), name='input_{0}'.format(num_var))\n",
    "            inputs.append(input_num)\n",
    "            output_num.append(input_num)\n",
    "        output_num = tfkl.Concatenate(name='concatenate_num')(output_num)\n",
    "        #output_num = tfkl.BatchNormalization()(output_num)\n",
    "\n",
    "    else:\n",
    "        input_num = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(numeric_features[0]))\n",
    "        inputs.append(input_num)\n",
    "        output_num = input_num\n",
    "\n",
    "    # create an embedding for every categorical feature\n",
    "    for categorical_var in cat_feats:\n",
    "        # should me nunique() but events are poorly preprocessed\n",
    "        no_of_unique_cat = cardinality[categorical_var]\n",
    "        print(categorical_var, no_of_unique_cat)\n",
    "        embedding_size = 10  # min(np.ceil((no_of_unique_cat)/2), 10)\n",
    "        embedding_size = int(embedding_size)\n",
    "        vocab = no_of_unique_cat+1\n",
    "        # functionnal loop\n",
    "        input_cat = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(categorical_var))\n",
    "        inputs.append(input_cat)\n",
    "        embedding = tfkl.Embedding(\n",
    "            vocab, embedding_size, name='embedding_{0}'.format(\n",
    "                categorical_var),\n",
    "            embeddings_initializer='random_uniform'\n",
    "        )(input_cat)\n",
    "        vec = tfkl.Flatten(name='flatten_{0}'.format(\n",
    "            categorical_var))(embedding)\n",
    "\n",
    "        output_cat.append(vec)\n",
    "    output_cat = tfkl.Concatenate(name='concatenate_cat')(output_cat)\n",
    "\n",
    "    # concatenate numerical input and embedding output\n",
    "    dense = tfkl.Concatenate(name='concatenate_all')([output_num, output_cat])\n",
    "\n",
    "    for i in range(len(layers_list)):\n",
    "        dense = tfp.layers.DenseVariational(layers_list[i],\n",
    "                                            activation='elu',\n",
    "                                            name='Dense_{0}'.format(str(i)),\n",
    "                                            make_posterior_fn=posterior_mean_field,\n",
    "                                            make_prior_fn=prior_trainable,\n",
    "                                            kl_weight=1/2048,\n",
    "                                            )(dense)\n",
    "        dense = tfp.bijectors.BatchNormalization()(dense)\n",
    "\n",
    "    if type_output == 'gaussian':\n",
    "        dense2 = tfp.layers.DenseVariational(2,\n",
    "                                             #activation = 'softplus',\n",
    "                                             make_posterior_fn=posterior_mean_field,\n",
    "                                             make_prior_fn=prior_trainable,\n",
    "                                             kl_weight=1/2048,\n",
    "                                             )(dense)\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.LogNormal(tf.math.softplus(0.05 * t[..., 0]),\n",
    "                                    scale=1e-6 + tf.math.softplus(0.05 * t[..., 1:])))(dense2)\n",
    "    # Poisson\n",
    "    elif type_output == 'poisson':\n",
    "        dense2 = tfp.layers.DenseVariational(2,\n",
    "                                             posterior_mean_field,\n",
    "                                             prior_trainable,\n",
    "                                             # kl_weight=1/100000\n",
    "                                             )(dense)\n",
    "        dense2 = tfp.layers.DenseFlipout(2)(dense)\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Poisson(rate=tf.math.softplus(t[..., 0])))(dense2)\n",
    "\n",
    "    # Gamma\n",
    "    elif type_output == 'gamma':\n",
    "        #         dense2 = tfp.layers.DenseFlipout(2,\n",
    "        #                                   name='Output',\n",
    "        #                                   activation = 'softplus',\n",
    "        # #                                   kernel_prior =prior_trainable,\n",
    "        # #                                  kernel_divergence_fn=kl_divergence_function,\n",
    "        #                                  )(dense)\n",
    "        dense2 = tfp.layers.DenseVariational(2,\n",
    "                                             name='Output',\n",
    "                                             activation='linear',\n",
    "                                             make_posterior_fn=posterior_mean_field,\n",
    "                                             kl_weight=1/2048,\n",
    "\n",
    "                                             make_prior_fn=prior_trainable,\n",
    "                                             )(dense)\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Gamma(concentration=t[..., 0],\n",
    "                                rate=t[..., 1]))(dense2)\n",
    "\n",
    "    else:\n",
    "        output = tfk.Dense(1, kernel_divergence_fn=kl_divergence_function,\n",
    "                           name='Output')(dense)\n",
    "\n",
    "    model = tfk.Model(inputs, output)\n",
    "    opt = tfk.optimizers.Adam(learning_rate=1e-3)\n",
    "    # kl divergence is direclty added to the loss function\n",
    "    model.compile(loss=neg_log_likelihood_continuous, optimizer=opt,\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
