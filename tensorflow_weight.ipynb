{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('./tf_pipeline/')\n",
    "\n",
    "from tf_pipeline.tf_utils import create_mlp_softmax\n",
    "\n",
    "from tf_pipeline.conf import *\n",
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def create_dt(horizon=\"validation\", tr_last=1913):\n",
    "    prices = pd.read_csv(os.path.join(RAW_PATH, \"sell_prices.csv\"), dtype=PRICE_DTYPES)\n",
    "    for col, col_dtype in PRICE_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            prices[col] = prices[col].cat.codes.astype(\"int16\")\n",
    "            prices[col] -= prices[col].min()\n",
    "\n",
    "    cal = pd.read_csv(os.path.join(RAW_PATH, \"calendar.csv\"), dtype=CAL_DTYPES)\n",
    "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
    "    for col, col_dtype in CAL_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            cal[col] = cal[col].cat.codes.astype(\"int16\")\n",
    "            cal[col] -= cal[col].min()\n",
    "\n",
    "    numcols = [f\"d_{day}\" for day in range(1, tr_last + 1)]\n",
    "    catcols = [\"id\", \"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\n",
    "    dtype = {numcol: \"float32\" for numcol in numcols}\n",
    "    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n",
    "    dt = pd.read_csv(\n",
    "        os.path.join(RAW_PATH, \"sales_train_%s.csv\" % horizon),\n",
    "        usecols=catcols + numcols,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    for col in catcols:\n",
    "        if col != \"id\":\n",
    "            dt[col] = dt[col].cat.codes.astype(\"int16\")\n",
    "            dt[col] -= dt[col].min()\n",
    "\n",
    "    increasing_term = dt.groupby([\"dept_id\", \"store_id\"])[numcols].sum()\n",
    "    increasing_term = (\n",
    "                              increasing_term.T - increasing_term.T.shift(28)\n",
    "                      ) / increasing_term.T.shift(28)\n",
    "    increasing_term = increasing_term.reset_index(drop=True).iloc[-365:, :]\n",
    "    rates = increasing_term[increasing_term.abs() < 1].mean() + 1\n",
    "    rates = rates.reset_index().rename(columns={0: \"rate\"})\n",
    "\n",
    "    for day in range(tr_last + 1, tr_last + 2 * 28 + 1):\n",
    "        dt[f\"d_{day}\"] = np.nan\n",
    "\n",
    "    dt = pd.melt(\n",
    "        dt,\n",
    "        id_vars=catcols,\n",
    "        value_vars=[col for col in dt.columns if col.startswith(\"d_\")],\n",
    "        var_name=\"d\",\n",
    "        value_name=\"sales\",\n",
    "    )\n",
    "\n",
    "    dt = dt.merge(cal, on=\"d\", copy=False)\n",
    "    dt = dt.merge(prices, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], copy=False)\n",
    "    dt = dt.merge(rates, how=\"left\")\n",
    "\n",
    "    return dt\n",
    "\n",
    "def compute_share(dt):\n",
    "    shares = (\n",
    "        dt.groupby([\"dept_id\", \"store_id\", \"date\"])[\"sales\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"sales\": \"gp_sales\"})\n",
    "    )\n",
    "    dt = dt.merge(shares, how=\"left\")\n",
    "    dt[\"sales\"] = dt[\"sales\"] / dt[\"gp_sales\"]\n",
    "    dt.drop([\"gp_sales\"], axis=1, inplace=True)\n",
    "    return dt\n",
    "\n",
    "def create_fea(dt):\n",
    "    lags = [7, 28]\n",
    "    lag_cols = [f\"lag_{lag}\" for lag in lags]\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        dt[lag_col] = dt[[\"id\", \"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
    "\n",
    "    wins = [7, 28]\n",
    "    for win in wins:\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            dt[f\"rmean_{lag}_{win}\"] = (\n",
    "                dt[[\"id\", lag_col]]\n",
    "                    .groupby(\"id\")[lag_col]\n",
    "                    .transform(lambda x: x.rolling(win).mean())\n",
    "            )\n",
    "\n",
    "    date_features = {\n",
    "        \"wday\": \"weekday\",\n",
    "        \"week\": \"weekofyear\",\n",
    "        \"month\": \"month\",\n",
    "        \"quarter\": \"quarter\",\n",
    "        \"year\": \"year\",\n",
    "        \"mday\": \"day\",\n",
    "    }\n",
    "\n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        if date_feat_name in dt.columns:\n",
    "            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n",
    "        else:\n",
    "            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")\n",
    "    return dt\n",
    "\n",
    "def train_and_pred(horizon=\"validation\"):\n",
    "\n",
    "    if horizon==\"validation\":\n",
    "        tr_last = 1913\n",
    "        fday = datetime(2016, 4, 25)\n",
    "    elif horizon==\"evaluation\":\n",
    "        tr_last = 1941\n",
    "        fday = datetime(2016, 4, 25) + timedelta(days=28)\n",
    "    else:\n",
    "        raise ValueError('Wrong horizon arg.')\n",
    "\n",
    "    dataframe = create_dt(horizon, tr_last)\n",
    "    dataframe = compute_share(dataframe)\n",
    "    dataframe = create_fea(dataframe)\n",
    "    dataframe.dropna(inplace=True)\n",
    "    \n",
    "\n",
    "    list_preds = list()\n",
    "\n",
    "    for _, df_gp in dataframe.groupby(['store_id', 'dept_id']):\n",
    "\n",
    "        cat_feats = ['wday', 'quarter']\n",
    "\n",
    "        n_items = len(df_gp['item_id'].drop_duplicates())    \n",
    "\n",
    "        ids = df_gp[['id', 'item_id']].drop_duplicates()\\\n",
    "                                      .sort_values('item_id')['id']\\\n",
    "                                      .tolist()\n",
    "        X = df_gp[\n",
    "            ['d', 'item_id', 'wday', 'quarter', 'date', 'rmean_28_28', 'sales']\n",
    "        ].pivot_table(index=['d', 'date', 'wday', 'quarter'],\n",
    "                      columns=['item_id'],\n",
    "                      values=['rmean_28_28', 'sales']).fillna(0)\n",
    "\n",
    "        num_feats = ['_'.join(list(map(str, c))) for c in X.columns.tolist()]\n",
    "        X.columns = num_feats\n",
    "\n",
    "        target_feats = num_feats[n_items:]\n",
    "        num_feats = num_feats[:n_items]\n",
    "        X = X.reset_index()\n",
    "\n",
    "        X_train = X_train = X[(X['date']<fday) & (X['date']>=fday - timedelta(days=364))][num_feats+cat_feats]\n",
    "        X_test = X[X['date']>=fday][num_feats+cat_feats]\n",
    "\n",
    "        input_dict_train = {'input_%s' % c: X_train[c] for c in num_feats+cat_feats}\n",
    "        input_dict_test = {'input_%s' % c: X_test[c] for c in num_feats+cat_feats}\n",
    "\n",
    "        cardinality = X[cat_feats].nunique()\n",
    "\n",
    "        y_train = X[X['date']<fday][target_feats].values\n",
    "\n",
    "        mlp = create_mlp_softmax(layers_list=[2048, 2048],\n",
    "                                 output_count=n_items,\n",
    "                                 cat_feats=cat_feats,\n",
    "                                 cardinality=cardinality,\n",
    "                                 num_feats=num_feats)\n",
    "\n",
    "\n",
    "\n",
    "        training_params = {\n",
    "                    'x': input_dict_train,\n",
    "                    'y': y_train,\n",
    "                    'batch_size': 128,\n",
    "                    'epochs': 20,\n",
    "                    'shuffle': True,\n",
    "                }\n",
    "\n",
    "        mlp.fit(**training_params)\n",
    "        preds = mlp.predict(input_dict_test)\n",
    "        preds = pd.DataFrame(preds,\n",
    "                             index=['F%s' % c for c in range(1,29)],\n",
    "                             columns=ids).T\n",
    "        list_preds.append(preds)\n",
    "\n",
    "    preds = pd.concat(list_preds)\n",
    "    preds = preds.reset_index()\n",
    "    preds.columns = ['id'] + preds.columns.tolist()[1:]\n",
    "    \n",
    "    preds.to_csv(\n",
    "            os.path.join(EXTERNAL_PATH, \"tf_weights_%s.csv\" % horizon), index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-00e84febf8f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_share\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_fea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-48c80db19241>\u001b[0m in \u001b[0;36mcreate_fea\u001b[0;34m(dt)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlag_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlag_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             )\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DataProjects/m5-forts-castors/.venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DataProjects/m5-forts-castors/.venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3000\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3001\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DataProjects/m5-forts-castors/.venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3612\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3613\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreindexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DataProjects/m5-forts-castors/.venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreindexer\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   3592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3594\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3595\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "horizon=\"validation\"\n",
    "\n",
    "if horizon==\"validation\":\n",
    "    tr_last = 1913\n",
    "    fday = datetime(2016, 4, 25)\n",
    "elif horizon==\"evaluation\":\n",
    "    tr_last = 1941\n",
    "    fday = datetime(2016, 4, 25) + timedelta(days=28)\n",
    "else:\n",
    "    raise ValueError('Wrong horizon arg.')\n",
    "\n",
    "dataframe = create_dt(horizon, tr_last)\n",
    "dataframe = compute_share(dataframe)\n",
    "dataframe = create_fea(dataframe)\n",
    "dataframe.dropna(inplace=True)\n",
    "\n",
    "\n",
    "list_preds = list()\n",
    "\n",
    "for _, df_gp in dataframe.groupby(['store_id', 'dept_id']):\n",
    "\n",
    "    cat_feats = ['wday', 'quarter']\n",
    "\n",
    "    n_items = len(df_gp['item_id'].drop_duplicates())    \n",
    "\n",
    "    ids = df_gp[['id', 'item_id']].drop_duplicates()\\\n",
    "                                  .sort_values('item_id')['id']\\\n",
    "                                  .tolist()\n",
    "    X = df_gp[\n",
    "        ['d', 'item_id', 'wday', 'quarter', 'date', 'rmean_28_28', 'sales']\n",
    "    ].pivot_table(index=['d', 'date', 'wday', 'quarter'],\n",
    "                  columns=['item_id'],\n",
    "                  values=['rmean_28_28', 'sales']).fillna(0)\n",
    "\n",
    "    num_feats = ['_'.join(list(map(str, c))) for c in X.columns.tolist()]\n",
    "    X.columns = num_feats\n",
    "\n",
    "    target_feats = num_feats[n_items:]\n",
    "    num_feats = num_feats[:n_items]\n",
    "    X = X.reset_index()\n",
    "\n",
    "    X_train = X_train = X[(X['date']<fday) & (X['date']>=fday - timedelta(days=364))][num_feats+cat_feats]\n",
    "    X_test = X[X['date']>=fday][num_feats+cat_feats]\n",
    "\n",
    "    input_dict_train = {'input_%s' % c: X_train[c] for c in num_feats+cat_feats}\n",
    "    input_dict_test = {'input_%s' % c: X_test[c] for c in num_feats+cat_feats}\n",
    "\n",
    "    cardinality = X[cat_feats].nunique()\n",
    "\n",
    "    y_train = X[X['date']<fday][target_feats].values\n",
    "\n",
    "    mlp = create_mlp_softmax(layers_list=[2048, 2048],\n",
    "                             output_count=n_items,\n",
    "                             cat_feats=cat_feats,\n",
    "                             cardinality=cardinality,\n",
    "                             num_feats=num_feats)\n",
    "\n",
    "\n",
    "\n",
    "    training_params = {\n",
    "                'x': input_dict_train,\n",
    "                'y': y_train,\n",
    "                'batch_size': 128,\n",
    "                'epochs': 20,\n",
    "                'shuffle': True,\n",
    "            }\n",
    "\n",
    "    mlp.fit(**training_params)\n",
    "    preds = mlp.predict(input_dict_test)\n",
    "    preds = pd.DataFrame(preds,\n",
    "                         index=['F%s' % c for c in range(1,29)],\n",
    "                         columns=ids).T\n",
    "    list_preds.append(preds)\n",
    "\n",
    "preds = pd.concat(list_preds)\n",
    "preds = preds.reset_index()\n",
    "preds.columns = ['id'] + preds.columns.tolist()[1:]\n",
    "\n",
    "preds.to_csv(\n",
    "        os.path.join(EXTERNAL_PATH, \"tf_weights_%s.csv\" % horizon), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
