{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, pickle, psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "# Merging by concat to not lose dtypes\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/raw/sales_train_evaluation.csv')\n",
    "prices_df = pd.read_csv('data/raw/sell_prices.csv')\n",
    "calendar_df = pd.read_csv('data/raw/calendar.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Grid (train_df in long format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save original id order using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete train_df to include (unknown) evaluation sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reindex(columns=train_df.columns.tolist() + ['d_' + str(1942 + i) for i in range(28)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melt train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = pd.melt(train_df, \n",
    "                  id_vars = ['index', 'id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "                  var_name = 'd', \n",
    "                  value_name = 'sales')\n",
    "\n",
    "print('Nb train rows:', len(train_df), 'to', len(grid_df))\n",
    "\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:>20}: {:>8}\".format('Original grid_df', sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# Convert categoricals\n",
    "for col in ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Reduce numerics\n",
    "grid_df['sales'] = grid_df['sales'].astype(np.float16)\n",
    "\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df', sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:>20}: {:>8}\".format('Original prices_df', sizeof_fmt(prices_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# Convert categoricals\n",
    "for col in ['store_id', 'item_id']:\n",
    "    prices_df[col] = prices_df[col].astype('category')\n",
    "    \n",
    "# Reduce numerics\n",
    "prices_df['wm_yr_wk'] = prices_df['wm_yr_wk'].astype(np.int16)\n",
    "prices_df['sell_price'] = prices_df['sell_price'].astype(np.float32) # not float16 for keeping the same precision\n",
    "\n",
    "print(\"{:>20}: {:>8}\".format('Reduced prices_df', sizeof_fmt(prices_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:>20}: {:>8}\".format('Original calendar_df', sizeof_fmt(calendar_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# Convert categoricals\n",
    "cat_cols = ['weekday', 'event_name_1', 'event_type_1',\n",
    "            'event_name_2', 'event_type_2','snap_CA',\n",
    "            'snap_TX', 'snap_WI']\n",
    "    \n",
    "for col in cat_cols:\n",
    "    calendar_df[col] = calendar_df[col].astype('category')\n",
    "\n",
    "# Reduce numerics\n",
    "calendar_df['wm_yr_wk'] = calendar_df['wm_yr_wk'].astype(np.int16)\n",
    "calendar_df['wday'] = calendar_df['wday'].astype(np.int8)\n",
    "calendar_df['month'] = calendar_df['month'].astype(np.int8)\n",
    "calendar_df['year'] = calendar_df['year'].astype(np.int16)\n",
    "\n",
    "\n",
    "print(\"{:>20}: {:>8}\".format('Reduced calendar_df', sizeof_fmt(calendar_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Grid size', grid_df.shape)\n",
    "\n",
    "grid_df = merge_by_concat(grid_df, calendar_df, merge_on=['d'])\n",
    "grid_df = merge_by_concat(grid_df, prices_df, merge_on=['store_id', 'item_id', 'wm_yr_wk'])\n",
    "\n",
    "print('Final Grid size', grid_df.shape)\n",
    "\n",
    "del calendar_df, prices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additionnal cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncate TS before release date (the first filled sell price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Grid size', grid_df.shape)\n",
    "\n",
    "grid_df = grid_df.dropna(subset=['sell_price']).reset_index(drop=True)\n",
    "\n",
    "print('Final Grid size', grid_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert 'd' to int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with NaN on categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_df.isna().sum()\n",
    "\n",
    "for col in ['event_name_1', 'event_name_2', 'event_type_1', 'event_type_2']:\n",
    "    grid_df[col] = grid_df[col].cat.add_categories('None')\n",
    "    grid_df[col].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = grid_df.sort_values(['index', 'd']).reset_index(drop=True)\n",
    "grid_df.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as interim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pickle to not lose dtypes\n",
    "grid_df.to_pickle('data/interim/grid_df.pkl')\n",
    "\n",
    "grid_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
