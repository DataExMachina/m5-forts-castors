{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENS_LEVEL_KEYS = ['store_id', 'dept_id']\n",
    "\n",
    "LIST_ALGO = ['lgb_estim', \n",
    "             'tf_estim', \n",
    "             'Prophet_store_dpt_lgb_weights']\n",
    "\n",
    "OUTPUT_NAME = 'lgb_tf_prophet_ensembling'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load usefull data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_csv('data/raw/sales_train_validation.csv')\n",
    "ids = ids[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and melt forecast files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = []\n",
    "\n",
    "for algo in LIST_ALGO:\n",
    "    df = pd.read_csv('data/submission/' + algo + '_validation.csv')\n",
    "    df_melt = pd.melt(df[df['id'].str.contains('validation')], id_vars='id', value_name=algo)\n",
    "    melted_df.append(df_melt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and melt solution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = pd.read_csv('data/raw/sales_train_evaluation.csv')[['id'] + ['d_%s' % c for c in range(1914, 1942)]]\n",
    "true.columns = ['id'] + ['F%s' % c for c in range(1, 29)]\n",
    "true['id'] = true['id'].str.replace('evaluation', 'validation')\n",
    "true = pd.melt(true, id_vars='id', value_name='true')\n",
    "melted_df.append(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all & add neutral forecast (only 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = reduce(pd.merge, melted_df)\n",
    "errors = pd.merge(ids, errors)\n",
    "errors['neutral'] = 0\n",
    "errors.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate ensembling weights & apply them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ensemble = list()\n",
    "weights_dict = {}\n",
    "\n",
    "for _, gp in errors.groupby(ENS_LEVEL_KEYS):\n",
    "    errors_gp = list()\n",
    "    \n",
    "    for m in LIST_ALGO + ['neutral']:\n",
    "        errors_gp.append(np.sqrt(np.mean(np.square(gp['true'] - gp[m]))))\n",
    "    \n",
    "    e0 = errors_gp[-1] # neutral error\n",
    "    es = errors_gp[:len(LIST_ALGO)] # algo error\n",
    "    ps = gp[LIST_ALGO].values\n",
    "    \n",
    "    l = 0.0001\n",
    "    m = len(es)\n",
    "    n = len(ps)\n",
    "    X = ps\n",
    "    pTy = 0.5 * (n * e0**2 + (X**2).sum(axis=0) - n * np.array(es)**2)\n",
    "    w = np.linalg.pinv(X.T.dot(X) + l * n * np.eye(m)).dot(pTy)\n",
    "    weights_dict[_] = w\n",
    "    \n",
    "    gp['ensemble'] = gp[LIST_ALGO].values.dot(w)\n",
    "    list_ensemble.append(gp)\n",
    "\n",
    "ensemble = pd.concat(list_ensemble)\n",
    "ensemble['ensemble'].clip(0, inplace=True)\n",
    "ensemble.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in LIST_ALGO + ['ensemble']:\n",
    "    print(m, np.sqrt(np.mean(np.square(ensemble['true'] - ensemble[m]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ensemble.pivot_table(index=['id'], columns=['variable'], values=['ensemble']).reset_index()\n",
    "ensemble.columns = [c[0] if c[1]=='' else c[1] for c in ensemble.columns.tolist()]\n",
    "ensemble = ensemble[['id'] + ['F%s' % c for c in range(1,29)]]\n",
    "ensemble.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save weights and validation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/external/weights_' + OUTPUT_NAME + '.pkl', 'wb')\n",
    "pickle.dump(weights_dict, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.to_csv('data/submission/' + OUTPUT_NAME + '_validation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
