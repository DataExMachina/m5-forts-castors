{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network regression using tensorflow\n",
    "author : Killian Pitiot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "from multiprocessing import Pool  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from conf import * \n",
    "from tf_utils import * \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfkl = tfk.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# custom imports\n",
    "warnings.filterwarnings('ignore')  # to avoid tfp bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python raw_data_prep.py\n",
    "# !python feature_engineering.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = \"validation\"\n",
    "task = 'volume'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "        os.path.join(REFINED_PATH, \"%s_%s_fe.parquet\" % (horizon, task))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAL_DTYPES = {\"event_name_1\": \"category\",\n",
    "#               \"event_name_2\": \"category\",\n",
    "#               \"event_type_1\": \"category\",\n",
    "#               \"event_type_2\": \"category\",\n",
    "#               \"weekday\": \"category\",\n",
    "#               'wm_yr_wk': 'int16',\n",
    "#               \"wday\": \"int16\",\n",
    "#               \"month\": \"int16\",\n",
    "#               \"year\": \"int16\",\n",
    "#               \"snap_CA\": \"float32\",\n",
    "#               'snap_TX': 'float32',\n",
    "#               'snap_WI': 'float32'}\n",
    "\n",
    "# PRICE_DTYPES = {\"store_id\": \"category\",\n",
    "#                 \"item_id\": \"category\",\n",
    "#                 \"wm_yr_wk\": \"int16\",\n",
    "#                 \"sell_price\": \"float32\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training constants \n",
    "# use_sampled_ds = False # does not work yet  \n",
    "# h = 28 \n",
    "# max_lags = 366\n",
    "# tr_last = 1913#1941 for the new cutoff\n",
    "# fday = datetime(2016,4, 25) \n",
    "\n",
    "# preprocess_interim()\n",
    "# df =pd.read_csv('./data/interim/df_tf_valid_.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading data  \n",
    "\n",
    "# sales_train_validation = pd.read_csv('./data/raw/sales_train_validation.csv')\n",
    "# increasing_term = sales_train_validation.groupby(['dept_id', 'store_id'])\\\n",
    "#                 [['d_%s' % c for c in range(1,1914)]].sum()\n",
    "\n",
    "# increasing_term = (increasing_term.T - increasing_term.T.shift(28))/increasing_term.T.shift(28)\n",
    "# increasing_term = increasing_term.reset_index(drop=True).iloc[-365:,:]\n",
    "\n",
    "# rates = increasing_term[increasing_term.abs()<1].mean()+1\n",
    "# rates = rates.reset_index().rename(columns={0:'rate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dt(is_train=True, nrows=None, first_day=1200):\n",
    "    prices = pd.read_csv(\"./data/raw/sell_prices.csv\", dtype=PRICE_DTYPES)\n",
    "    for col, col_dtype in PRICE_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            prices[col] = prices[col].cat.codes.astype(\"int16\")\n",
    "            prices[col] -= prices[col].min()\n",
    "\n",
    "    cal = pd.read_csv(\"./data/raw/calendar.csv\", dtype=CAL_DTYPES)\n",
    "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
    "    for col, col_dtype in CAL_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            cal[col] = cal[col].cat.codes.astype(\"int16\")\n",
    "            cal[col] -= cal[col].min()\n",
    "\n",
    "    start_day = max(1 if is_train else tr_last-max_lags, first_day)\n",
    "    numcols = [f\"d_{day}\" for day in range(start_day, tr_last+1)]\n",
    "    catcols = ['id', 'item_id', 'dept_id', 'store_id', 'cat_id', 'state_id']\n",
    "    dtype = {numcol: \"float32\" for numcol in numcols}\n",
    "    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n",
    "    dt = pd.read_csv(\"./data/raw/sales_train_validation.csv\",\n",
    "                     nrows=nrows, usecols=catcols + numcols, dtype=dtype)\n",
    "\n",
    "    for col in catcols:\n",
    "        if col != \"id\":\n",
    "            dt[col] = dt[col].cat.codes.astype(\"int16\")\n",
    "            dt[col] -= dt[col].min()\n",
    "\n",
    "    increasing_term = dt.groupby(['dept_id', 'store_id'])[numcols].sum()\n",
    "    increasing_term = (increasing_term.T -\n",
    "                       increasing_term.T.shift(28))/increasing_term.T.shift(28)\n",
    "    increasing_term = increasing_term.reset_index(drop=True).iloc[-365:, :]\n",
    "    rates = inreasing_term[increasing_term.abs() < 1].mean()+1\n",
    "    rates = rates.reset_index().rename(columns={0: 'rate'})\n",
    "\n",
    "    if not is_train:\n",
    "        for day in range(tr_last+1, tr_last + 2*h + 1):\n",
    "            dt[f\"d_{day}\"] = np.nan\n",
    "\n",
    "    dt = pd.melt(dt,\n",
    "                 id_vars=catcols,\n",
    "                 value_vars=[\n",
    "                     col for col in dt.columns if col.startswith(\"d_\")],\n",
    "                 var_name=\"d\",\n",
    "                 value_name=\"sales\")\n",
    "\n",
    "    dt = dt.merge(cal, on=\"d\", copy=False)\n",
    "    dt = dt.merge(prices, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], copy=False)\n",
    "    dt = dt.merge(rates, how='left')\n",
    "\n",
    "    return dt\n",
    "\n",
    "\n",
    "# def create_features(dt):\n",
    "#     lags = [7, 28]\n",
    "#     lag_cols = [f\"lag_{lag}\" for lag in lags]\n",
    "#     for lag, lag_col in zip(lags, lag_cols):\n",
    "#         dt[lag_col] = dt[[\"id\", \"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
    "\n",
    "#     wins = [7, 28]\n",
    "#     for win in wins:\n",
    "#         for lag, lag_col in zip(lags, lag_cols):\n",
    "#             dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\n",
    "#                 \"id\")[lag_col].transform(lambda x: x.rolling(win).mean())\n",
    "\n",
    "#     date_features = {\n",
    "\n",
    "#         \"wday\": \"weekday\",\n",
    "#         \"week\": \"weekofyear\",\n",
    "#         \"month\": \"month\",\n",
    "#         \"quarter\": \"quarter\",\n",
    "#         \"year\": \"year\",\n",
    "#         \"mday\": \"day\",\n",
    "#     }\n",
    "\n",
    "#     for date_feat_name, date_feat_func in date_features.items():\n",
    "#         if date_feat_name in dt.columns:\n",
    "#             dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n",
    "#         else:\n",
    "#             dt[date_feat_name] = getattr(\n",
    "#                 dt[\"date\"].dt, date_feat_func).astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST_DAY = 1 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tfk.callbacks.Callback):\n",
    "    def __init__(self, print_n_epochs=10):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.print_n_epochs = print_n_epochs\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch%self.print_n_epochs==0:\n",
    "            try:\n",
    "                print(f\"Epoch {epoch} loss : {logs['loss']:.2f}, rmse :{logs['root_mean_squared_error']:.2f}, val_rmse :{logs['val_root_mean_squared_error']:.2f}\")\n",
    "            except:\n",
    "                print(f\"Epoch {epoch} loss : {logs['loss']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df = create_dt(is_train=True, first_day= FIRST_DAY)\n",
    "# create_features(df)\n",
    "# df.dropna(inplace = True)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weights for the training (the older the sample the less it will have impact )\n",
    "weights = df['d'].str[2:].astype(int)\n",
    "weights = weights/np.sum(weights)\n",
    "\n",
    "# weights based on the amplitude of the sales\n",
    "p_sampling = df['sales']/df['sales'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_feats = ['item_id',\n",
    "#              'dept_id',\n",
    "#              'store_id',\n",
    "#              'cat_id',\n",
    "#              'state_id'] + \\\n",
    "#             [\"event_name_1\",\n",
    "#              \"event_name_2\",\n",
    "#              \"event_type_1\",\n",
    "#              \"event_type_2\"]\n",
    "\n",
    "# useless_cols = [\"id\",\n",
    "#                 \"date\",\n",
    "#                 \"sales\",\n",
    "#                 \"d\",\n",
    "#                 \"wm_yr_wk\",\n",
    "#                 \"weekday\"\n",
    "#                ]\n",
    "\n",
    "input_dict,y_train, input_dict_test, y_test, cardinality, weights_train = df_to_tf(df, cat_feats, useless_cols, use_validation=True)\n",
    "\n",
    "# num_feats = df.columns[~df.columns.isin(useless_cols+cat_feats)].to_list()\n",
    "# train_cols = num_feats+cat_feats\n",
    "\n",
    "# X_train = df[train_cols]\n",
    "# y_train = df[\"sales\"]\n",
    "\n",
    "# np.random.seed(777)\n",
    "\n",
    "# fake_valid_inds = np.random.choice(\n",
    "#     X_train.index.values, 2_000_000, replace=False)\n",
    "# train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n",
    "\n",
    "# X_test, y_test = X_train.loc[fake_valid_inds], y_train.loc[fake_valid_inds]\n",
    "# X_train, y_train = X_train.loc[train_inds], y_train.loc[train_inds]\n",
    "\n",
    "# X_train[num_feats], X_test[num_feats] = X_train[num_feats].astype(\n",
    "#     np.float32), X_test[num_feats].astype(np.float32)\n",
    "\n",
    "# X_train[cat_feats], X_test[cat_feats] = X_train[cat_feats].astype(\n",
    "#     np.int32), X_test[cat_feats].astype(np.int32)\n",
    "\n",
    "# cardinality = df[cat_feats].nunique()\n",
    "# weights_train = weights.loc[X_train.index]\n",
    "# p_sampling_train = p_sampling.loc[X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input for tensorflow\n",
    "# as we have multiple input type the best solution is to feed a dict like object\n",
    "\n",
    "# input_dict = {f\"input_{col}\": X_train[col] for col in X_train.columns}\n",
    "\n",
    "# input_dict_test = {f\"input_{col}\": X_test[col] for col in X_train.columns}\n",
    "\n",
    "# del df, X_train, X_test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of target unique values \n",
    "if use_sampled_ds:\n",
    "    tf_ds = tf.data.Dataset.from_tensor_slices((input_dict,{'target' : y_train.values}))\n",
    "    tf_ds = tf_ds.shuffle(5000).repeat()\n",
    "    ## list for all different sales (out of memory)\n",
    "    target_unique =np.unique(y_train.values)\n",
    "    weigth_sampling = [1/len(target_unique) for target_ in target_unique]\n",
    "    list_tf_ds = [(tf_ds.filter(lambda features, target: target==target_value))\\\n",
    "             for target_value in target_unique]\n",
    "    \n",
    "    ##  make stratified sampling \n",
    "    ## list for all different sales (out of memory)\n",
    "#     tf1 = tf_ds.filter(lambda features, target: target==target_value)\n",
    "#     weigth_sampling = [target_/np.sum(target_unique) for target_ in target_unique]\n",
    "#     list_tf_ds = [(tf_ds.filter(lambda features, target: target==target_value)).repeat()\\\n",
    "#              for target_value in target_unique]\n",
    "    \n",
    "    tf_ds = tf.data.experimental.sample_from_datasets(list_tf_ds,weigth_sampling)\n",
    "    tf_ds = tf_ds.take(2).batch(8,drop_remainder=True).prefetch(2)\n",
    "#     del input_dict \n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the tensorflow modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=C:\\Users\\Killian\\Documents\\python_code\\Kaggle\\m5-forecasting-accuracy\\logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TF2 model\n",
    "# # loss for a poisson regression\n",
    "\n",
    "\n",
    "# def poisson(y_true, y_pred):\n",
    "#     '''\n",
    "#     Loss computed as a Poisson regression \n",
    "#     '''\n",
    "#     return K.mean(K.maximum(.0, y_pred) - y_true * K.log(K.maximum(.0, y_pred) + K.epsilon()), axis=-1)\n",
    "\n",
    "\n",
    "# def tweedie_loss(y_true, y_pred):\n",
    "#     '''\n",
    "#     Tweedie regression, same style as poisson but ... well ... different \n",
    "\n",
    "#     '''\n",
    "#     p = 1.5\n",
    "#     dev = K.pow(y_true, 2-p)/((1-p) * (2-p)) \\\n",
    "#           - y_true * K.pow(K.maximum(.0, y_pred) + K.epsilon(), 1-p)/(1-p) \\\n",
    "#           + K.pow(K.maximum(.0, y_pred) + K.epsilon(), 2-p)/(2-p)\n",
    "\n",
    "#     return K.mean(dev, axis=-1)\n",
    "\n",
    "\n",
    "# alpha = .5\n",
    "\n",
    "\n",
    "# def weighted_loss(y_true, y_pred):\n",
    "#     '''\n",
    "#     make a comprised loss of poisson and tweedie distribution\n",
    "#     '''\n",
    "#     return (1 - alpha) * poisson(y_true, y_pred) + alpha * tweedie(y_true, y_pred)\n",
    "\n",
    "\n",
    "# # function to generate the MLP\n",
    "# def create_mlp(layers_list=[128, 128, 64, 64,32], # [512, 256, 128, 64] \n",
    "#                emb_dim=30,\n",
    "#                loss_fn='poisson',\n",
    "#                learning_rate=1e-3,\n",
    "#                optimizer=tfk.optimizers.Adam,\n",
    "#                verbose=0):\n",
    "#     '''\n",
    "#     description : \n",
    "#     generate regression mlp with\n",
    "#     both embedding entries for categorical features and \n",
    "#     standard inputs for numerical features\n",
    "\n",
    "#     params:\n",
    "#     layers_list : list of layers dimensions \n",
    "#     emb_dim : maximum embedding size\n",
    "#     output :\n",
    "#     uncompiled keras model  \n",
    "#     '''\n",
    "\n",
    "#     # define our MLP network\n",
    "#     layers = []\n",
    "#     output_num = []\n",
    "#     inputs = []\n",
    "#     output_cat = []\n",
    "#     output_num = []\n",
    "\n",
    "#     # sequencial inputs\n",
    "\n",
    "#     # numerical data part\n",
    "#     if len(num_feats) > 1:\n",
    "#         for num_var in num_feats:\n",
    "#             input_num = tfkl.Input(\n",
    "#                 shape=(1,), name='input_{0}'.format(num_var))\n",
    "#             inputs.append(input_num)\n",
    "#             output_num.append(input_num)\n",
    "#         output_num = tfkl.Concatenate(name='concatenate_num')(output_num)\n",
    "#         output_num = tfkl.BatchNormalization()(output_num) # to avoid preprocessing \n",
    "\n",
    "#     else:\n",
    "#         input_num = tfkl.Input(\n",
    "#             shape=(1,), name='input_{0}'.format(numeric_features[0]))\n",
    "#         inputs.append(input_num)\n",
    "#         output_num = input_num\n",
    "\n",
    "#     # categorical data input\n",
    "#     for categorical_var in cat_feats:\n",
    "#         no_of_unique_cat = cardinality[categorical_var]\n",
    "#         if verbose == 1:\n",
    "#             print(fcategorical_var, no_of_unique_cat)\n",
    "#         embedding_size = min(np.ceil((no_of_unique_cat)/2), emb_dim)\n",
    "#         embedding_size = int(embedding_size)\n",
    "#         vocab = no_of_unique_cat+1\n",
    "        \n",
    "#         # functionnal loop\n",
    "#         input_cat = tfkl.Input(\n",
    "#             shape=(1,), name='input_{0}'.format(categorical_var))\n",
    "#         inputs.append(input_cat)\n",
    "#         embedding = tfkl.Embedding(vocab,\n",
    "#                                    embedding_size,\n",
    "#                                    embeddings_regularizer = tf.keras.regularizers.l1(1e-8),\n",
    "#                                    name='embedding_{0}'.format(categorical_var))(input_cat)\n",
    "#         embedding = tfkl.Dropout(0.15)(embedding)\n",
    "#         vec = tfkl.Flatten(name='flatten_{0}'.format(\n",
    "#             categorical_var))(embedding)\n",
    "#         output_cat.append(vec)\n",
    "#     output_cat = tfkl.Concatenate(name='concatenate_cat')(output_cat)\n",
    "    \n",
    "#     # concatenate numerical input and embedding output\n",
    "#     dense = tfkl.Concatenate(name='concatenate_all')([output_num, output_cat])\n",
    "\n",
    "#     # dense network \n",
    "#     for i in range(len(layers_list)):\n",
    "#         dense = tfkl.Dense(layers_list[i],\n",
    "#                            name='Dense_{0}'.format(str(i)),\n",
    "#                            activation='elu')(dense)\n",
    "#         dense = tfkl.Dropout(.15)(dense)\n",
    "#         dense = tfkl.BatchNormalization()(dense)\n",
    "\n",
    "#     dense2 = tfkl.Dense(1, name='Output', activation='elu')(dense)\n",
    "#     model = tfk.Model(inputs, dense2)\n",
    "\n",
    "    \n",
    "#     opt = optimizer(learning_rate)\n",
    "    \n",
    "#     # choose the type of regression \n",
    "#     if loss_fn == 'poisson':\n",
    "#         model.compile(loss=poisson, optimizer=opt, metrics=[\n",
    "#                       tf.keras.metrics.RootMeanSquaredError()])\n",
    "#     elif loss_fn == 'tweedie':\n",
    "#         model.compile(loss=tweedie_loss, optimizer=opt, metrics=[\n",
    "#                       tf.keras.metrics.RootMeanSquaredError()])\n",
    "#     else:\n",
    "#         raise ValueError(\n",
    "#             \"Loss function should be either Poisson or tweedie for now\")\n",
    "#     return model\n",
    "\n",
    "# # remove the computing graph if exist (maybe no more required if TF2)\n",
    "# try:\n",
    "#     del mdl\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "mdl = create_mlp(layers_list=None,\n",
    "           emb_dim=30,\n",
    "           loss_fn='tweedie',\n",
    "           learning_rate=1e-3,\n",
    "           optimizer=tfk.optimizers.Adam, \n",
    "           cat_feats=cat_feats,\n",
    "           num_feats=num_feats,\n",
    "           cardinality=cardinality,\n",
    "           verbose=0)\n",
    "\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training of the algorithm \n",
    "\n",
    "# checkpointsthe model to reload the best parameters\n",
    "model_save = tfk.callbacks.ModelCheckpoint('model_checkpoints',\n",
    "                                           verbose=0)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tfk.callbacks.EarlyStopping('val_root_mean_squared_error',\n",
    "                                             patience=10,\n",
    "                                             verbose=0,\n",
    "                                             restore_best_weights=True)\n",
    "\n",
    "# Logging for the tensorboard following\n",
    "log_dir = \"logs\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# # Tensorbard callback\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n",
    "#                                                       histogram_freq=1)\n",
    "\n",
    "\n",
    "\n",
    "## fit tf dataset and batch sampling  \n",
    "if use_sampled_ds:\n",
    "    # training with a custom loop \n",
    "    history = mdl.fit(tf_ds,\n",
    "                      validation_data=(input_dict_test, y_test.values),\n",
    "                      epochs=10,\n",
    "                      shuffle=True,\n",
    "                      steps_per_epoch = 1000, # as we are using sampling \n",
    "                      callbacks=[CustomCallback(1)],\n",
    "                      verbose=0\n",
    "                      )\n",
    "else:\n",
    "    # training with the weighted input dict \n",
    "    history = mdl.fit(input_dict,\n",
    "                      y_train.values,\n",
    "                      validation_data=(input_dict_test, y_test.values),\n",
    "                      batch_size=4096,\n",
    "                      epochs=100,\n",
    "                      shuffle=True,\n",
    "                      sample_weight=weights_train.values, # weighting the grads\n",
    "                      callbacks=[model_save, early_stopping,CustomCallback(1)],\n",
    "                      verbose=0,\n",
    "                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.save('keras_tweedie.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['root_mean_squared_error'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model rmse')\n",
    "plt.ylabel('rmse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig('poisson.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make optimization for the NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports we know we'll need only for BGS \n",
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.callbacks import CheckpointSaver, VerboseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start to generate the search space for the gaussian process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the optimization of HP functions using skopt\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate,\n",
    "            num_dense_layers,\n",
    "            num_dense_nodes,\n",
    "            batch_size,\n",
    "            emb_dim,\n",
    "            loss_fn,\n",
    "            ):\n",
    "\n",
    "    # generate a list of the MLP architecture\n",
    "    # Enforce a funnel like structure\n",
    "    list_layer = [num_dense_nodes//(2**x) for x in range(num_dense_layers)]\n",
    "\n",
    "    model = create_mlp(layers_list=list_layer,\n",
    "                       emb_dim=emb_dim,\n",
    "                       loss_fn='poisson',\n",
    "                       learning_rate=learning_rate,\n",
    "                       optimizer=tfk.optimizers.Adam\n",
    "                       )\n",
    "\n",
    "    print(\n",
    "        f'Generated a model with {model.count_params()} trainable parameters')\n",
    "\n",
    "    # checkpointsthe model to reload the best parameters\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree('./model_checkpoints')\n",
    "        print('Old checkpoint remove')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    model_save = tfk.callbacks.ModelCheckpoint('model_checkpoints',verbose=0)\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = tfk.callbacks.EarlyStopping('val_root_mean_squared_error',\n",
    "                                                 patience=10,\n",
    "                                                 verbose = 0,\n",
    "                                                 restore_best_weights=True)\n",
    "    history = model.fit(input_dict,\n",
    "                        y_train.values,\n",
    "                        validation_data=(input_dict_test, y_test.values),\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=100,\n",
    "                        shuffle=True,\n",
    "                        # importance of the samples based of the ages\n",
    "                        sample_weight=weights_train.values,\n",
    "                        callbacks=[model_save, early_stopping,CustomCallback(10)],\n",
    "                        verbose=0,\n",
    "                        )\n",
    "    # return the validation accuracy for the last epoch.\n",
    "    rmse = history.history['val_root_mean_squared_error'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"RMSE: {:.2}\".format(rmse))\n",
    "    print()\n",
    "\n",
    "    global best_rmse\n",
    "    if rmse < best_rmse:\n",
    "        model.save('keras_best_model.h5')\n",
    "        best_rmse = rmse\n",
    "    print(\"Best RMSE: {:.2}\".format(rmse))\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory(not sure if needed in tf2)\n",
    "    del model, early_stopping, model_save\n",
    "    # Clear the Keras session\n",
    "    K.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    gc.collect()\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the optimization interuptible\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "checkpoint_callback = skopt.callbacks.CheckpointSaver(\"./result.pkl\")\n",
    "\n",
    "best_rmse = 1000\n",
    "gp_result = gp_minimize(func=fitness,\n",
    "                        dimensions=dimensions,\n",
    "                        n_calls=20,\n",
    "                        n_jobs=1,\n",
    "                        verbose=True,\n",
    "                        callback=[checkpoint_callback],\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gp_result.best_estimator_)\n",
    "print(gp_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_objective\n",
    "_ = plot_convergence(gp_result)\n",
    "_ = plot_objective(gp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the forecast results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "alphas = [1.035, 1.03, 1.025]\n",
    "weights = [1/len(alphas)]*len(alphas)\n",
    "sub = 0.\n",
    "\n",
    "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
    "\n",
    "    te = create_dt(False)\n",
    "    cols = [f\"F{i}\" for i in range(1, 29)]\n",
    "\n",
    "    for tdelta in range(0, 28):\n",
    "        day = fday + timedelta(days=tdelta)\n",
    "        print(icount, day)\n",
    "        tst = te[(te.date >= day - timedelta(days=max_lags))\n",
    "                 & (te.date <= day)].copy()\n",
    "        create_fea(tst)\n",
    "        tst = tst.loc[tst.date == day, train_cols]\n",
    "        input_dict_predict = {f\"input_{col}\": tst[col] for col in train_cols}\n",
    "        pred = mdl.predict(input_dict_predict, batch_size=10000)\n",
    "        te.loc[te.date == day, \"sales\"] = alpha*pred\n",
    "        print(pred)\n",
    "\n",
    "    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
    "#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h),\n",
    "#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\n",
    "        \"id\"].cumcount()+1]\n",
    "    te_sub = te_sub.set_index([\"id\", \"F\"]).unstack()[\n",
    "        \"sales\"][cols].reset_index()\n",
    "    te_sub.fillna(0., inplace=True)\n",
    "    te_sub.sort_values(\"id\", inplace=True)\n",
    "    te_sub.reset_index(drop=True, inplace=True)\n",
    "    te_sub.to_csv(f\"submission_{icount}.csv\", index=False)\n",
    "    if icount == 0:\n",
    "        sub = te_sub\n",
    "        sub[cols] *= weight\n",
    "    else:\n",
    "        sub[cols] += te_sub[cols]*weight\n",
    "    print(icount, alpha, weight)\n",
    "\n",
    "\n",
    "sub2 = sub.copy()\n",
    "sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it similar to lightgbm outputing \n",
    "%%time\n",
    "\n",
    "te = create_dt(False)\n",
    "te.shape\n",
    "\n",
    "for i in range(0, 28):\n",
    "    day = fday + timedelta(days=i)\n",
    "    print(i, day)\n",
    "    tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n",
    "    create_fea(tst)\n",
    "    tst = tst.loc[tst.date == day , train_cols.tolist()+['rate']]\n",
    "    input_dict_predict = {f\"input_{col}\": tst[col] for col in tst.columns}\n",
    "    te.loc[te.date == day, \"sales\"] = tst['rate']*mdl.predict(tst[train_cols],batch_size=10000) #1.02*m_lgb.predict(tst[train_cols]) # magic multiplier by kyakovlev\n",
    "    \n",
    "te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
    "te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n",
    "                                                                      \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][[f\"F{i}\" for i in range(1,29)]].reset_index()\n",
    "te_sub.fillna(0., inplace = True)\n",
    "te_sub.to_csv(\"tensorflow_predictions.csv\",index=False)\n",
    "te_sub.shape    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFP  : modify training and loss to adapt it to bayesian training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "\n",
    "# prior distribution on the weights \n",
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t, scale=1),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "# layer posterior distribution with mean field approximation  \n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1.))\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t[..., :n],\n",
    "                       scale=1e-5 + 0.02*tf.nn.softplus(c + t[..., n:])),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "# probabilistic  loss functions\n",
    "def neg_log_likelihood_continuous(y_true, y_pred):\n",
    "    '''\n",
    "    negative log likelyhood for stricly positive distributions \n",
    "    \n",
    "    '''\n",
    "    return -y_pred.prob(y_true+1e-6)\n",
    "\n",
    "\n",
    "def neg_log_likelihood_discrete(y_true, y_pred):\n",
    "    '''\n",
    "    negloglik when we don't care about the probability in zero \n",
    "    '''\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "# custom metrics\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    '''\n",
    "    compact implemtation of the rmse \n",
    "    '''\n",
    "    return tf.math.sqrt(tf.math.reduce_mean(tf.math.square(y_pred-y_true)))\n",
    "\n",
    "\n",
    "# to much scaling: high variance, no enough -> converge to the likelyhood\n",
    "kl_weight = batch_size/training_size \n",
    "kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda\n",
    "                          kl_weight)  # KL over the batch of 2048 (should normalize by the number of mini batch)\n",
    "\n",
    "\n",
    "def create_mlp(layers_list=[512, 512, 512, 64],\n",
    "               type_output='poisson'):\n",
    "    '''\n",
    "    description : \n",
    "    generate regression mlp with\n",
    "    both embedding entries for categorical features and \n",
    "    standard inputs for numerical features\n",
    "\n",
    "    params:\n",
    "    layers_list : list of layers dimensions \n",
    "    output :\n",
    "    compiled keras model  \n",
    "    '''\n",
    "\n",
    "    # define our MLP network\n",
    "    layers = []\n",
    "    output_num = []\n",
    "    inputs = []\n",
    "    output_cat = []\n",
    "    output_num = []\n",
    "\n",
    "    # numerical data part\n",
    "    if len(num_feats) > 1:\n",
    "        for num_var in num_feats:\n",
    "            input_num = tfkl.Input(\n",
    "                shape=(1,), name='input_{0}'.format(num_var))\n",
    "            inputs.append(input_num)\n",
    "            output_num.append(input_num)\n",
    "        output_num = tfkl.Concatenate(name='concatenate_num')(output_num)\n",
    "        #output_num = tfkl.BatchNormalization()(output_num)\n",
    "\n",
    "    else:\n",
    "        input_num = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(numeric_features[0]))\n",
    "        inputs.append(input_num)\n",
    "        output_num = input_num\n",
    "\n",
    "    # create an embedding for every categorical feature\n",
    "    for categorical_var in cat_feats:\n",
    "        # should me nunique() but events are poorly preprocessed\n",
    "        no_of_unique_cat = cardinality[categorical_var]\n",
    "        print(categorical_var, no_of_unique_cat)\n",
    "        embedding_size = min(np.ceil((no_of_unique_cat)/2), 30)\n",
    "        embedding_size = int(embedding_size)\n",
    "        vocab = no_of_unique_cat+1\n",
    "        # functionnal loop\n",
    "        input_cat = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(categorical_var))\n",
    "        inputs.append(input_cat)\n",
    "        embedding = tfkl.Embedding(vocab,\n",
    "                                   embedding_size,\n",
    "                                   embeddings_regularizer = tf.keras.regularizers.l1(1e-8),\n",
    "                                   name='embedding_{0}'.format(categorical_var))(input_cat)\n",
    "        embedding = tfkl.Dropout(0.1)(embedding)\n",
    "        vec = tfkl.Flatten(name='flatten_{0}'.format(\n",
    "            categorical_var))(embedding)\n",
    "        output_cat.append(vec)\n",
    "    output_cat = tfkl.Concatenate(name='concatenate_cat')(output_cat)\n",
    "\n",
    "    # concatenate numerical input and embedding output\n",
    "    dense = tfkl.Concatenate(name='concatenate_all')([output_num, output_cat])\n",
    "\n",
    "    for i in range(len(layers_list)):\n",
    "        dense = tfkl.Dense(layers_list[i],\n",
    "                           name='Dense_{0}'.format(str(i)),\n",
    "                           activation='elu')(dense)\n",
    "        dense = tfkl.Dropout(.1)(dense)\n",
    "        dense = tfkl.BatchNormalization()(dense)\n",
    "\n",
    "    # lognormal\n",
    "    if type_output == 'gaussian':\n",
    "        dense2 = tfk.layers.Dense(2,\n",
    "                                  activation='softplus',\n",
    "                                  name='Output'\n",
    "                                  )(dense)\n",
    "\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Normal(t[..., 0], scale=t[..., 1:]))(dense2)\n",
    "\n",
    "    # Poisson\n",
    "    elif type_output == 'poisson':\n",
    "        dense2 = tfk.layers.Dense(1,\n",
    "                                  name='Output')(dense)\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Poisson(rate=tf.math.softplus(t[..., 0])))(dense2)\n",
    "\n",
    "    # Gamma\n",
    "    elif type_output == 'gamma':\n",
    "        dense2 = tfk.layers.Dense(2,\n",
    "                                  name='Output',\n",
    "                                  activation='softplus',\n",
    "                                  )(dense)\n",
    "\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Gamma(concentration=.01*t[..., 0],\n",
    "                                rate=.01*t[..., 1]))(dense2)\n",
    "    else:\n",
    "        output = tfk.Dense(1)\n",
    "\n",
    "    model = tfk.Model(inputs, output)\n",
    "    opt = tfk.optimizers.Nadam(learning_rate=1e-2)\n",
    "\n",
    "    model.compile(loss=neg_log_likelihood_continuous,\n",
    "                  optimizer=opt,\n",
    "                  metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "\n",
    "try:\n",
    "    del mdl_tfp\n",
    "    print('model deleted ')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "mdl_tfp = create_mlp(type_output='poisson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "history = mdl_tfp.fit(input_dict,\n",
    "                      y_train.values,\n",
    "                      validation_data=(input_dict_test, y_test.values),\n",
    "                      batch_size=4096,\n",
    "                      epochs=2,\n",
    "                      shuffle=True,\n",
    "                      verbose=1,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a custom random sampled training loop\n",
    "# # need to call the model with training = True for the custom training to handle dropout and BN\n",
    "\n",
    "# def random_batch(input_X,y,batch_size=128,sample_weights=None):\n",
    "#     # take a sample batch based on the weighting\n",
    "#     idx = np.random.choice(range(len(input_X)),sample_weights,batch_size)\n",
    "#     return input_X[idx],y[idx]\n",
    "\n",
    "\n",
    "# model = create_mlp()\n",
    "\n",
    "# # initialoze parameters\n",
    "# n_epochs = 5\n",
    "# batch_size = 2048\n",
    "# n_steps = 50\n",
    "# opt = tfk.optimizers.Adam(learning_rate=1e-3)\n",
    "# loss_fn = poisson\n",
    "# mean_loss = tfk.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fully bayesian network\n",
    "\n",
    "def create_mlp_full_bayes(layers_list=[512, 256, 128, 64],\n",
    "                          type_output='poisson'):\n",
    "    '''\n",
    "    description : \n",
    "    generate regression mlp with\n",
    "    both embedding entries for categorical features and \n",
    "    standard inputs for numerical features\n",
    "\n",
    "    params:\n",
    "    layers_list : list of layers dimensions \n",
    "    output :\n",
    "    compiled keras model  \n",
    "    '''\n",
    "\n",
    "    # define our MLP network\n",
    "    layers = []\n",
    "    output_num = []\n",
    "    inputs = []\n",
    "    output_cat = []\n",
    "    output_num = []\n",
    "\n",
    "    # numerical data part\n",
    "    if len(num_feats) > 1:\n",
    "        for num_var in num_feats:\n",
    "            input_num = tfkl.Input(\n",
    "                shape=(1,), name='input_{0}'.format(num_var))\n",
    "            inputs.append(input_num)\n",
    "            output_num.append(input_num)\n",
    "        output_num = tfkl.Concatenate(name='concatenate_num')(output_num)\n",
    "        #output_num = tfkl.BatchNormalization()(output_num)\n",
    "\n",
    "    else:\n",
    "        input_num = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(numeric_features[0]))\n",
    "        inputs.append(input_num)\n",
    "        output_num = input_num\n",
    "\n",
    "    # create an embedding for every categorical feature\n",
    "    for categorical_var in cat_feats:\n",
    "        # should me nunique() but events are poorly preprocessed\n",
    "        no_of_unique_cat = cardinality[categorical_var]\n",
    "        print(categorical_var, no_of_unique_cat)\n",
    "        embedding_size = 10  # min(np.ceil((no_of_unique_cat)/2), 10)\n",
    "        embedding_size = int(embedding_size)\n",
    "        vocab = no_of_unique_cat+1\n",
    "        # functionnal loop\n",
    "        input_cat = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(categorical_var))\n",
    "        inputs.append(input_cat)\n",
    "        embedding = tfkl.Embedding(\n",
    "            vocab, embedding_size, name='embedding_{0}'.format(\n",
    "                categorical_var),\n",
    "            embeddings_initializer='random_uniform'\n",
    "        )(input_cat)\n",
    "        vec = tfkl.Flatten(name='flatten_{0}'.format(\n",
    "            categorical_var))(embedding)\n",
    "\n",
    "        output_cat.append(vec)\n",
    "    output_cat = tfkl.Concatenate(name='concatenate_cat')(output_cat)\n",
    "\n",
    "    # concatenate numerical input and embedding output\n",
    "    dense = tfkl.Concatenate(name='concatenate_all')([output_num, output_cat])\n",
    "\n",
    "    for i in range(len(layers_list)):\n",
    "        dense = tfp.layers.DenseVariational(layers_list[i],\n",
    "                                            activation='elu',\n",
    "                                            name='Dense_{0}'.format(str(i)),\n",
    "                                            make_posterior_fn=posterior_mean_field,\n",
    "                                            make_prior_fn=prior_trainable,\n",
    "                                            kl_weight=1/2048,\n",
    "                                            )(dense)\n",
    "        dense = tfp.bijectors.BatchNormalization()(dense)\n",
    "\n",
    "    if type_output == 'gaussian':\n",
    "        dense2 = tfp.layers.DenseVariational(2,\n",
    "                                             #activation = 'softplus',\n",
    "                                             make_posterior_fn=posterior_mean_field,\n",
    "                                             make_prior_fn=prior_trainable,\n",
    "                                             kl_weight=1/2048,\n",
    "                                             )(dense)\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.LogNormal(tf.math.softplus(0.05 * t[..., 0]),\n",
    "                                    scale=1e-6 + tf.math.softplus(0.05 * t[..., 1:])))(dense2)\n",
    "    # Poisson\n",
    "    elif type_output == 'poisson':\n",
    "        dense2 = tfp.layers.DenseVariational(2,\n",
    "                                             posterior_mean_field,\n",
    "                                             prior_trainable,\n",
    "                                             # kl_weight=1/100000\n",
    "                                             )(dense)\n",
    "        dense2 = tfp.layers.DenseFlipout(2)(dense)\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Poisson(rate=tf.math.softplus(t[..., 0])))(dense2)\n",
    "\n",
    "    # Gamma\n",
    "    elif type_output == 'gamma':\n",
    "        #         dense2 = tfp.layers.DenseFlipout(2,\n",
    "        #                                   name='Output',\n",
    "        #                                   activation = 'softplus',\n",
    "        # #                                   kernel_prior =prior_trainable,\n",
    "        # #                                  kernel_divergence_fn=kl_divergence_function,\n",
    "        #                                  )(dense)\n",
    "        dense2 = tfp.layers.DenseVariational(2,\n",
    "                                             name='Output',\n",
    "                                             activation='linear',\n",
    "                                             make_posterior_fn=posterior_mean_field,\n",
    "                                             kl_weight=1/2048,\n",
    "\n",
    "                                             make_prior_fn=prior_trainable,\n",
    "                                             )(dense)\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Gamma(concentration=t[..., 0],\n",
    "                                rate=t[..., 1]))(dense2)\n",
    "\n",
    "    else:\n",
    "        output = tfk.Dense(1, kernel_divergence_fn=kl_divergence_function,\n",
    "                           name='Output')(dense)\n",
    "\n",
    "    model = tfk.Model(inputs, output)\n",
    "    opt = tfk.optimizers.Adam(learning_rate=1e-3)\n",
    "    # kl divergence is direclty added to the loss function\n",
    "    model.compile(loss=neg_log_likelihood_continuous, optimizer=opt,\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make  a custom batch generator \n",
    "based on https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tfk.utils.Sequence):\n",
    "    'Generates data for tfk'\n",
    "\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32, 32, 32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # X : (n_samples, *dim, n_channels)\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i, ] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
