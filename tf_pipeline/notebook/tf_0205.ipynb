{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "from multiprocessing import Pool  # Multiprocess Runs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfkl = tfk.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# custom imports\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL_DTYPES = {\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\",\n",
    "              \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n",
    "              \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32'}\n",
    "PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\",\n",
    "                \"wm_yr_wk\": \"int16\", \"sell_price\": \"float32\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 28\n",
    "max_lags = 57\n",
    "tr_last = 1913\n",
    "fday = datetime.datetime(2016, 4, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dt(is_train=True, nrows=None, first_day=1200):\n",
    "    prices = pd.read_csv(\"../data/raw/sell_prices.csv\", dtype=PRICE_DTYPES)\n",
    "    for col, col_dtype in PRICE_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            prices[col] = prices[col].cat.codes.astype(\"int16\")\n",
    "            prices[col] -= prices[col].min()\n",
    "\n",
    "    cal = pd.read_csv(\"../data/raw/calendar.csv\", dtype=CAL_DTYPES)\n",
    "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
    "    for col, col_dtype in CAL_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            cal[col] = cal[col].cat.codes.astype(\"int16\")\n",
    "            cal[col] -= cal[col].min()\n",
    "\n",
    "    start_day = max(1 if is_train else tr_last-max_lags, first_day)\n",
    "    numcols = [f\"d_{day}\" for day in range(start_day, tr_last+1)]\n",
    "    catcols = ['id', 'item_id', 'dept_id', 'store_id', 'cat_id', 'state_id']\n",
    "    dtype = {numcol: \"float32\" for numcol in numcols}\n",
    "    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n",
    "    dt = pd.read_csv(\"../data/raw/sales_train_validation.csv\",\n",
    "                     nrows=nrows, usecols=catcols + numcols, dtype=dtype)\n",
    "\n",
    "    for col in catcols:\n",
    "        if col != \"id\":\n",
    "            dt[col] = dt[col].cat.codes.astype(\"int16\")\n",
    "            dt[col] -= dt[col].min()\n",
    "\n",
    "    if not is_train:\n",
    "        for day in range(tr_last+1, tr_last + 28 + 1):\n",
    "            dt[f\"d_{day}\"] = np.nan\n",
    "\n",
    "    dt = pd.melt(dt,\n",
    "                 id_vars=catcols,\n",
    "                 value_vars=[\n",
    "                     col for col in dt.columns if col.startswith(\"d_\")],\n",
    "                 var_name=\"d\",\n",
    "                 value_name=\"sales\")\n",
    "\n",
    "    dt = dt.merge(cal, on=\"d\", copy=False)\n",
    "    dt = dt.merge(prices, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], copy=False)\n",
    "\n",
    "    return dt\n",
    "\n",
    "\n",
    "def create_fea(dt):\n",
    "    lags = [7, 28]\n",
    "    lag_cols = [f\"lag_{lag}\" for lag in lags]\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        dt[lag_col] = dt[[\"id\", \"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
    "\n",
    "    wins = [7, 28]\n",
    "    for win in wins:\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\n",
    "                \"id\")[lag_col].transform(lambda x: x.rolling(win).mean())\n",
    "\n",
    "    date_features = {\n",
    "\n",
    "        \"wday\": \"weekday\",\n",
    "        \"week\": \"weekofyear\",\n",
    "        \"month\": \"month\",\n",
    "        \"quarter\": \"quarter\",\n",
    "        \"year\": \"year\",\n",
    "        \"mday\": \"day\",\n",
    "        #         \"ime\": \"is_month_end\",\n",
    "        #         \"ims\": \"is_month_start\",\n",
    "    }\n",
    "\n",
    "    #     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n",
    "\n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        if date_feat_name in dt.columns:\n",
    "            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n",
    "        else:\n",
    "            dt[date_feat_name] = getattr(\n",
    "                dt[\"date\"].dt, date_feat_func).astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow utils functions\n",
    "class CustomCallbackLargeDataset(tfk.callbacks.Callback):\n",
    "    def __init__(self, print_n_batch=1000, print_n_epoch=1):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.print_n_epoch = print_n_epoch\n",
    "        self.print_n_batch = print_n_batch\n",
    "        self.seen = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.step += 1\n",
    "        for k in self.params['metrics']:\n",
    "            if k in logs:\n",
    "                self.metric_cache[k] = self.metric_cache.get(k, 0) + logs[k]\n",
    "        if self.step % self.display == 0:\n",
    "            metrics_log = ''\n",
    "            for (k, v) in self.metric_cache.items():\n",
    "                val = v / self.display\n",
    "                if abs(val) > 1e-3:\n",
    "                    metrics_log += ' - %s: %.4f' % (k, val)\n",
    "                else:\n",
    "                    metrics_log += ' - %s: %.4e' % (k, val)\n",
    "            print('step: {}/{} ... {}'.format(self.step,\n",
    "                                              self.params['steps'],\n",
    "                                              metrics_log))\n",
    "            self.metric_cache.clear()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.print_n_epochs == 0:\n",
    "            try:\n",
    "                print(\n",
    "                    f\"Epoch {epoch} loss : {logs['loss']:.2f}, mse :{logs['root_mean_squared_error']:.2f}\")\n",
    "            except:\n",
    "                print(f\"Epoch {epoch} loss : {logs['loss']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_DAY = 350  # 350 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !\n",
    "df = create_dt(is_train=True, first_day=FIRST_DAY)\n",
    "create_fea(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of the dataframe : 4.29453959GB \n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory usage of the dataframe : {df.memory_usage().sum()/1e9}GB \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weights for the training (the older the sample the less it will have impact )\n",
    "weights = df['d'].str[2:].astype(int)\n",
    "weights = weights/np.sum(weights)\n",
    "\n",
    "# weights based on the amplitude of the sales\n",
    "p_sampling = df['sales']/df['sales'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = ['item_id', 'dept_id', 'store_id', 'cat_id', 'state_id'] + \\\n",
    "    [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
    "useless_cols = [\"id\", \"date\", \"sales\", \"d\", \"wm_yr_wk\", \"weekday\"]\n",
    "num_feats = df.columns[~df.columns.isin(useless_cols+cat_feats)].to_list()\n",
    "train_cols = num_feats+cat_feats\n",
    "\n",
    "X_train = df[train_cols]\n",
    "y_train = df[\"sales\"]\n",
    "\n",
    "np.random.seed(777)\n",
    "fake_valid_inds = np.random.choice(\n",
    "    X_train.index.values, 2_000_000, replace=False)\n",
    "train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n",
    "\n",
    "X_test, y_test = X_train.loc[fake_valid_inds], y_train.loc[fake_valid_inds]\n",
    "X_train, y_train = X_train.loc[train_inds], y_train.loc[train_inds]\n",
    "\n",
    "X_train[num_feats], X_test[num_feats] = X_train[num_feats].astype(\n",
    "    np.float32), X_test[num_feats].astype(np.float32)\n",
    "X_train[cat_feats], X_test[cat_feats] = X_train[cat_feats].astype(\n",
    "    np.int32), X_test[cat_feats].astype(np.int32)\n",
    "\n",
    "cardinality = df[cat_feats].max()\n",
    "weights_train = weights.loc[X_train.index]\n",
    "p_sampling_train = p_sampling.loc[X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare input for tensorflow\n",
    "# as we have multiple input type the best solution is to feed a dict like object\n",
    "\n",
    "input_dict = {f\"input_{col}\": X_train[col] for col in X_train.columns}\n",
    "input_dict_test = {f\"input_{col}\": X_test[col] for col in X_train.columns}\n",
    "\n",
    "del df, X_train, X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the tensorflow modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=C:\\Users\\Killian\\Documents\\python_code\\Kaggle\\m5-forecasting-accuracy\\logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id 3048\n",
      "dept_id 6\n",
      "store_id 9\n",
      "cat_id 2\n",
      "state_id 2\n",
      "event_name_1 30\n",
      "event_name_2 4\n",
      "event_type_1 4\n",
      "event_type_2 2\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_item_id (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_dept_id (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_store_id (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_cat_id (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_state_id (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_event_name_1 (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_event_name_2 (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_event_type_1 (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_event_type_2 (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_item_id (Embedding)   (None, 1, 30)        91470       input_item_id[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_dept_id (Embedding)   (None, 1, 3)         21          input_dept_id[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_store_id (Embedding)  (None, 1, 5)         50          input_store_id[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_cat_id (Embedding)    (None, 1, 1)         3           input_cat_id[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_state_id (Embedding)  (None, 1, 1)         3           input_state_id[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_event_name_1 (Embeddi (None, 1, 15)        465         input_event_name_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_event_name_2 (Embeddi (None, 1, 2)         10          input_event_name_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_event_type_1 (Embeddi (None, 1, 2)         10          input_event_type_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_event_type_2 (Embeddi (None, 1, 1)         3           input_event_type_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_wday (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_month (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_year (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_snap_CA (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_snap_TX (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_snap_WI (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_sell_price (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_lag_7 (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_lag_28 (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_rmean_7_7 (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_rmean_28_7 (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_rmean_7_28 (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_rmean_28_28 (InputLayer)  [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_week (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_quarter (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mday (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 1, 30)        0           embedding_item_id[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 1, 3)         0           embedding_dept_id[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1, 5)         0           embedding_store_id[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 1, 1)         0           embedding_cat_id[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 1, 1)         0           embedding_state_id[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 1, 15)        0           embedding_event_name_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 1, 2)         0           embedding_event_name_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 1, 2)         0           embedding_event_type_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 1, 1)         0           embedding_event_type_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_num (Concatenate)   (None, 16)           0           input_wday[0][0]                 \n",
      "                                                                 input_month[0][0]                \n",
      "                                                                 input_year[0][0]                 \n",
      "                                                                 input_snap_CA[0][0]              \n",
      "                                                                 input_snap_TX[0][0]              \n",
      "                                                                 input_snap_WI[0][0]              \n",
      "                                                                 input_sell_price[0][0]           \n",
      "                                                                 input_lag_7[0][0]                \n",
      "                                                                 input_lag_28[0][0]               \n",
      "                                                                 input_rmean_7_7[0][0]            \n",
      "                                                                 input_rmean_28_7[0][0]           \n",
      "                                                                 input_rmean_7_28[0][0]           \n",
      "                                                                 input_rmean_28_28[0][0]          \n",
      "                                                                 input_week[0][0]                 \n",
      "                                                                 input_quarter[0][0]              \n",
      "                                                                 input_mday[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_item_id (Flatten)       (None, 30)           0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_dept_id (Flatten)       (None, 3)            0           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_store_id (Flatten)      (None, 5)            0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_cat_id (Flatten)        (None, 1)            0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_state_id (Flatten)      (None, 1)            0           dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_event_name_1 (Flatten)  (None, 15)           0           dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_event_name_2 (Flatten)  (None, 2)            0           dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_event_type_1 (Flatten)  (None, 2)            0           dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_event_type_2 (Flatten)  (None, 1)            0           dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16)           64          concatenate_num[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_cat (Concatenate)   (None, 60)           0           flatten_item_id[0][0]            \n",
      "                                                                 flatten_dept_id[0][0]            \n",
      "                                                                 flatten_store_id[0][0]           \n",
      "                                                                 flatten_cat_id[0][0]             \n",
      "                                                                 flatten_state_id[0][0]           \n",
      "                                                                 flatten_event_name_1[0][0]       \n",
      "                                                                 flatten_event_name_2[0][0]       \n",
      "                                                                 flatten_event_type_1[0][0]       \n",
      "                                                                 flatten_event_type_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_all (Concatenate)   (None, 76)           0           batch_normalization_6[0][0]      \n",
      "                                                                 concatenate_cat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Dense_0 (Dense)                 (None, 128)          9856        concatenate_all[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 128)          0           Dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128)          512         dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dense_1 (Dense)                 (None, 64)           8256        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 64)           0           Dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64)           256         dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dense_2 (Dense)                 (None, 32)           2080        batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32)           0           Dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32)           128         dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dense_3 (Dense)                 (None, 16)           528         batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 16)           0           Dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16)           64          dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, 1)            17          batch_normalization_10[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 113,796\n",
      "Trainable params: 113,284\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TF2 model\n",
    "# loss for a poisson regression\n",
    "\n",
    "\n",
    "def poisson(y_true, y_pred):\n",
    "    '''\n",
    "    Loss computed as a Poisson regression \n",
    "    '''\n",
    "    return K.mean(K.maximum(.0, y_pred) - y_true * K.log(K.maximum(.0, y_pred) + K.epsilon()), axis=-1)\n",
    "\n",
    "\n",
    "def tweedie_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Tweedie regression, same style as poisson \n",
    "\n",
    "    '''\n",
    "    p = 1.5\n",
    "    dev = K.pow(y_true, 2-p)/((1-p) * (2-p)) \\\n",
    "        - y_true * K.pow(K.maximum(.0, y_pred) + K.epsilon(), 1-p)/(1-p) \\\n",
    "        + K.pow(K.maximum(.0, y_pred) + K.epsilon(), 2-p)/(2-p)\n",
    "    return K.mean(dev, axis=-1)\n",
    "\n",
    "\n",
    "alpha = .5\n",
    "\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    '''\n",
    "    make a comprised loss of poisson and tweedie distribution\n",
    "    '''\n",
    "    return (1 - alpha) * poisson(y_true, y_pred) + alpha * tweedie(y_true, y_pred)\n",
    "\n",
    "# function to generate the MLP\n",
    "\n",
    "\n",
    "def create_mlp(layers_list=[128, 64, 32, 16],\n",
    "               emb_dim=30,\n",
    "               loss_fn='poisson',\n",
    "               learning_rate=1e-3,\n",
    "               optimizer=tfk.optimizers.Adam,\n",
    "               verbose=1):\n",
    "    '''\n",
    "    description : \n",
    "    generate regression mlp with\n",
    "    both embedding entries for categorical features and \n",
    "    standard inputs for numerical features\n",
    "\n",
    "    params:\n",
    "    layers_list : list of layers dimensions \n",
    "    emb_dim : maximum embedding size\n",
    "    output :\n",
    "    uncompiled keras model  \n",
    "    '''\n",
    "\n",
    "    # define our MLP network\n",
    "    layers = []\n",
    "    output_num = []\n",
    "    inputs = []\n",
    "    output_cat = []\n",
    "    output_num = []\n",
    "\n",
    "    # sequencial inputs\n",
    "\n",
    "    # numerical data part\n",
    "    if len(num_feats) > 1:\n",
    "        for num_var in num_feats:\n",
    "            input_num = tfkl.Input(\n",
    "                shape=(1,), name='input_{0}'.format(num_var))\n",
    "            inputs.append(input_num)\n",
    "            output_num.append(input_num)\n",
    "        output_num = tfkl.Concatenate(name='concatenate_num')(output_num)\n",
    "        output_num = tfkl.BatchNormalization()(output_num)\n",
    "\n",
    "    else:\n",
    "        input_num = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(numeric_features[0]))\n",
    "        inputs.append(input_num)\n",
    "        output_num = input_num\n",
    "\n",
    "    # categorical data input\n",
    "    for categorical_var in cat_feats:\n",
    "        # should me nunique() but events are poorly preprocessed\n",
    "        no_of_unique_cat = cardinality[categorical_var]\n",
    "        if verbose == 1:\n",
    "            print(categorical_var, no_of_unique_cat)\n",
    "        embedding_size = min(np.ceil((no_of_unique_cat)/2), emb_dim)\n",
    "        embedding_size = int(embedding_size)\n",
    "        vocab = no_of_unique_cat+1\n",
    "        # functionnal loop\n",
    "        input_cat = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(categorical_var))\n",
    "        inputs.append(input_cat)\n",
    "        embedding = tfkl.Embedding(vocab,\n",
    "                                   embedding_size,\n",
    "                                   #embeddings_regularizer = tf.keras.regularizers.l1(1e-8),\n",
    "                                   name='embedding_{0}'.format(categorical_var))(input_cat)\n",
    "        embedding = tfkl.Dropout(0.1)(embedding)\n",
    "        vec = tfkl.Flatten(name='flatten_{0}'.format(\n",
    "            categorical_var))(embedding)\n",
    "\n",
    "        output_cat.append(vec)\n",
    "    output_cat = tfkl.Concatenate(name='concatenate_cat')(output_cat)\n",
    "\n",
    "    # concatenate numerical input and embedding output\n",
    "    dense = tfkl.Concatenate(name='concatenate_all')([output_num, output_cat])\n",
    "\n",
    "    for i in range(len(layers_list)):\n",
    "        dense = tfkl.Dense(layers_list[i],\n",
    "                           name='Dense_{0}'.format(str(i)),\n",
    "                           activation='elu')(dense)\n",
    "        dense = tfkl.Dropout(.1)(dense)\n",
    "        dense = tfkl.BatchNormalization()(dense)\n",
    "\n",
    "    output = tfkl.Dense(1, name='Output', activation='linear')(dense)\n",
    "    model = tfk.Model(inputs, output)\n",
    "\n",
    "    opt = optimizer(learning_rate)\n",
    "\n",
    "    if loss_fn == 'poisson':\n",
    "        model.compile(loss=poisson, optimizer=opt, metrics=[\n",
    "                      tf.keras.metrics.RootMeanSquaredError()])\n",
    "    elif loss_fn == 'tweedie':\n",
    "        model.compile(loss=tweedie_loss, optimizer=opt, metrics=[\n",
    "                      tf.keras.metrics.RootMeanSquaredError()])\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Loss function should be either Poisson or tweedie for now\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "try:\n",
    "    del mdl\n",
    "except:\n",
    "    pass\n",
    "\n",
    "mdl = create_mlp(loss_fn='poisson')\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 37041269 samples, validate on 2000000 samples\n",
      "Epoch 1/25\n",
      "37036032/37041269 [============================>.] - ETA: 0s - loss: 2.5756e-08 - root_mean_squared_error: 3.5673INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 95s 3us/sample - loss: 2.5753e-08 - root_mean_squared_error: 3.5672 - val_loss: -0.0753 - val_root_mean_squared_error: 3.3069\n",
      "Epoch 2/25\n",
      "37031936/37041269 [============================>.] - ETA: 0s - loss: 4.0289e-10 - root_mean_squared_error: 3.1705INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 92s 2us/sample - loss: 4.0230e-10 - root_mean_squared_error: 3.1705 - val_loss: -0.1737 - val_root_mean_squared_error: 3.0915\n",
      "Epoch 3/25\n",
      "37027840/37041269 [============================>.] - ETA: 0s - loss: -1.4913e-09 - root_mean_squared_error: 3.0306INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 91s 2us/sample - loss: -1.4915e-09 - root_mean_squared_error: 3.0305 - val_loss: -0.3020 - val_root_mean_squared_error: 2.9337\n",
      "Epoch 4/25\n",
      "37040128/37041269 [============================>.] - ETA: 0s - loss: -4.2490e-09 - root_mean_squared_error: 2.9104INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 89s 2us/sample - loss: -4.2490e-09 - root_mean_squared_error: 2.9104 - val_loss: -0.3339 - val_root_mean_squared_error: 2.8787\n",
      "Epoch 5/25\n",
      "37023744/37041269 [============================>.] - ETA: 0s - loss: -4.9046e-09 - root_mean_squared_error: 2.8730INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 89s 2us/sample - loss: -4.9058e-09 - root_mean_squared_error: 2.8731 - val_loss: -0.3589 - val_root_mean_squared_error: 2.8234\n",
      "Epoch 6/25\n",
      "37036032/37041269 [============================>.] - ETA: 0s - loss: -5.3759e-09 - root_mean_squared_error: 2.8346INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 89s 2us/sample - loss: -5.3761e-09 - root_mean_squared_error: 2.8346 - val_loss: -0.3666 - val_root_mean_squared_error: 2.7879\n",
      "Epoch 7/25\n",
      "37040128/37041269 [============================>.] - ETA: 0s - loss: -5.6393e-09 - root_mean_squared_error: 2.8110INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 90s 2us/sample - loss: -5.6395e-09 - root_mean_squared_error: 2.8110 - val_loss: -0.3681 - val_root_mean_squared_error: 2.7660\n",
      "Epoch 8/25\n",
      "37040128/37041269 [============================>.] - ETA: 0s - loss: -5.7855e-09 - root_mean_squared_error: 2.7939INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 89s 2us/sample - loss: -5.7852e-09 - root_mean_squared_error: 2.7940 - val_loss: -0.3732 - val_root_mean_squared_error: 2.7496\n",
      "Epoch 9/25\n",
      "37036032/37041269 [============================>.] - ETA: 0s - loss: -5.8663e-09 - root_mean_squared_error: 2.7830INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 89s 2us/sample - loss: -5.8665e-09 - root_mean_squared_error: 2.7832 - val_loss: -0.3644 - val_root_mean_squared_error: 2.7447\n",
      "Epoch 10/25\n",
      "37036032/37041269 [============================>.] - ETA: 0s - loss: -5.9157e-09 - root_mean_squared_error: 2.7732INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 88s 2us/sample - loss: -5.9155e-09 - root_mean_squared_error: 2.7731 - val_loss: -0.3802 - val_root_mean_squared_error: 2.7329\n",
      "Epoch 11/25\n",
      "37031936/37041269 [============================>.] - ETA: 0s - loss: -6.0532e-09 - root_mean_squared_error: 2.7632INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.0534e-09 - root_mean_squared_error: 2.7636 - val_loss: -0.3837 - val_root_mean_squared_error: 2.7213\n",
      "Epoch 12/25\n",
      "37036032/37041269 [============================>.] - ETA: 0s - loss: -6.1275e-09 - root_mean_squared_error: 2.7535INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.1281e-09 - root_mean_squared_error: 2.7535 - val_loss: -0.3812 - val_root_mean_squared_error: 2.7143\n",
      "Epoch 13/25\n",
      "37040128/37041269 [============================>.] - ETA: 0s - loss: -6.1531e-09 - root_mean_squared_error: 2.7468INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.1533e-09 - root_mean_squared_error: 2.7468 - val_loss: -0.3781 - val_root_mean_squared_error: 2.7124\n",
      "Epoch 14/25\n",
      "37023744/37041269 [============================>.] - ETA: 0s - loss: -6.2047e-09 - root_mean_squared_error: 2.7410INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.2059e-09 - root_mean_squared_error: 2.7412 - val_loss: -0.3843 - val_root_mean_squared_error: 2.6928\n",
      "Epoch 15/25\n",
      "37023744/37041269 [============================>.] - ETA: 0s - loss: -6.2865e-09 - root_mean_squared_error: 2.7333INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.2870e-09 - root_mean_squared_error: 2.7333 - val_loss: -0.3898 - val_root_mean_squared_error: 2.6915\n",
      "Epoch 16/25\n",
      "37040128/37041269 [============================>.] - ETA: 0s - loss: -6.3060e-09 - root_mean_squared_error: 2.7281INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.3059e-09 - root_mean_squared_error: 2.7281 - val_loss: -0.3928 - val_root_mean_squared_error: 2.6664\n",
      "Epoch 17/25\n",
      "37040128/37041269 [============================>.] - ETA: 0s - loss: -6.1768e-09 - root_mean_squared_error: 2.7253INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 85s 2us/sample - loss: -6.1772e-09 - root_mean_squared_error: 2.7253 - val_loss: -0.3830 - val_root_mean_squared_error: 2.6751\n",
      "Epoch 18/25\n",
      "37019648/37041269 [============================>.] - ETA: 0s - loss: -6.3836e-09 - root_mean_squared_error: 2.7186INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.3830e-09 - root_mean_squared_error: 2.7184 - val_loss: -0.3914 - val_root_mean_squared_error: 2.6811\n",
      "Epoch 19/25\n",
      "37036032/37041269 [============================>.] - ETA: 0s - loss: -6.4334e-09 - root_mean_squared_error: 2.7153INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.4333e-09 - root_mean_squared_error: 2.7152 - val_loss: -0.3945 - val_root_mean_squared_error: 2.6661\n",
      "Epoch 20/25\n",
      "37027840/37041269 [============================>.] - ETA: 0s - loss: -6.4714e-09 - root_mean_squared_error: 2.7104INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.4723e-09 - root_mean_squared_error: 2.7105 - val_loss: -0.3949 - val_root_mean_squared_error: 2.6661\n",
      "Epoch 21/25\n",
      "37031936/37041269 [============================>.] - ETA: 0s - loss: -6.4973e-09 - root_mean_squared_error: 2.7068INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.4973e-09 - root_mean_squared_error: 2.7068 - val_loss: -0.3970 - val_root_mean_squared_error: 2.6501\n",
      "Epoch 22/25\n",
      "37027840/37041269 [============================>.] - ETA: 0s - loss: -6.5222e-09 - root_mean_squared_error: 2.7041INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.5233e-09 - root_mean_squared_error: 2.7043 - val_loss: -0.3942 - val_root_mean_squared_error: 2.6609\n",
      "Epoch 23/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37040128/37041269 [============================>.] - ETA: 0s - loss: -6.5519e-09 - root_mean_squared_error: 2.7007INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.5522e-09 - root_mean_squared_error: 2.7007 - val_loss: -0.3994 - val_root_mean_squared_error: 2.6417\n",
      "Epoch 24/25\n",
      "37019648/37041269 [============================>.] - ETA: 0s - loss: -6.3548e-09 - root_mean_squared_error: 2.7039INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 85s 2us/sample - loss: -6.3540e-09 - root_mean_squared_error: 2.7037 - val_loss: -0.3980 - val_root_mean_squared_error: 2.6474\n",
      "Epoch 25/25\n",
      "37023744/37041269 [============================>.] - ETA: 0s - loss: -6.5767e-09 - root_mean_squared_error: 2.6972INFO:tensorflow:Assets written to: model_checkpoints\\assets\n",
      "37041269/37041269 [==============================] - 86s 2us/sample - loss: -6.5788e-09 - root_mean_squared_error: 2.6973 - val_loss: -0.4003 - val_root_mean_squared_error: 2.6355\n"
     ]
    }
   ],
   "source": [
    "# training of the algorithm \n",
    "\n",
    "# checkpointsthe model to reload the best parameters\n",
    "model_save = tfk.callbacks.ModelCheckpoint('model_checkpoints')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tfk.callbacks.EarlyStopping('val_root_mean_squared_error',\n",
    "                                             patience=10,\n",
    "                                             restore_best_weights=True)\n",
    "\n",
    "# Logging for the tensorboard following\n",
    "log_dir = \"logs\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# # Tensorbard callback\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n",
    "#                                                       histogram_freq=1)\n",
    "\n",
    "history = mdl.fit(input_dict,\n",
    "                  y_train.values,\n",
    "                  validation_data=(input_dict_test, y_test.values),\n",
    "                  batch_size=4096,\n",
    "                  epochs=25,\n",
    "                  shuffle=True,\n",
    "                  # importance of the samples based of the ages\n",
    "                  sample_weight=weights_train.values,\n",
    "                  callbacks=[model_save, early_stopping],\n",
    "                  verbose=1,\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.save('keras_tweedie.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['root_mean_squared_error'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model rmse')\n",
    "plt.ylabel('rmse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig('poisson.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "alphas = [1.035, 1.03, 1.025]\n",
    "weights = [1/len(alphas)]*len(alphas)\n",
    "sub = 0.\n",
    "\n",
    "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
    "\n",
    "    te = create_dt(False)\n",
    "    cols = [f\"F{i}\" for i in range(1, 29)]\n",
    "\n",
    "    for tdelta in range(0, 28):\n",
    "        day = fday + timedelta(days=tdelta)\n",
    "        print(icount, day)\n",
    "        tst = te[(te.date >= day - timedelta(days=max_lags))\n",
    "                 & (te.date <= day)].copy()\n",
    "        create_fea(tst)\n",
    "        tst = tst.loc[tst.date == day, train_cols]\n",
    "        input_dict_predict = {f\"input_{col}\": tst[col] for col in tst.columns}\n",
    "        pred = mdl.predict(input_dict_predict, batch_size=10000)\n",
    "        te.loc[te.date == day, \"sales\"] = alpha*pred\n",
    "        print(pred)\n",
    "\n",
    "    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
    "#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h),\n",
    "#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\n",
    "        \"id\"].cumcount()+1]\n",
    "    te_sub = te_sub.set_index([\"id\", \"F\"]).unstack()[\n",
    "        \"sales\"][cols].reset_index()\n",
    "    te_sub.fillna(0., inplace=True)\n",
    "    te_sub.sort_values(\"id\", inplace=True)\n",
    "    te_sub.reset_index(drop=True, inplace=True)\n",
    "    te_sub.to_csv(f\"submission_{icount}.csv\", index=False)\n",
    "    if icount == 0:\n",
    "        sub = te_sub\n",
    "        sub[cols] *= weight\n",
    "    else:\n",
    "        sub[cols] += te_sub[cols]*weight\n",
    "    print(icount, alpha, weight)\n",
    "\n",
    "\n",
    "sub2 = sub.copy()\n",
    "sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make optimization for the NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports we know we'll need only for BGS \n",
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.callbacks import CheckpointSaver, VerboseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start to generate the search space for the gaussian process\n",
    "\n",
    "dim_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_num_dense_layers = Integer(low=1, high=7, name='num_dense_layers')\n",
    "dim_num_dense_nodes = Integer(low=32, high=2048, name='num_dense_nodes')\n",
    "\n",
    "dim_batch_size = Integer(low=2048, high=10000, name='batch_size')\n",
    "dim_emb_dim = Integer(low=10, high=50, name='emb_dim')\n",
    "\n",
    "dim_loss_fn = Categorical(categories=['poisson', 'tweedie'], name='loss_fn')\n",
    "\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_emb_dim,\n",
    "              dim_batch_size,\n",
    "              dim_loss_fn,\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the optimization of HP functions using skopt\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate,\n",
    "            num_dense_layers,\n",
    "            num_dense_nodes,\n",
    "            batch_size,\n",
    "            emb_dim,\n",
    "            loss_fn,\n",
    "            ):\n",
    "\n",
    "    # generate a list of the MLP architecture\n",
    "    # Enforce a funnel like structure\n",
    "    list_layer = [num_dense_nodes//(2**x) for x in range(num_dense_layers)]\n",
    "\n",
    "    model = create_mlp(layers_list=list_layer,\n",
    "                       emb_dim=emb_dim,\n",
    "                       loss_fn='poisson',\n",
    "                       learning_rate=learning_rate,\n",
    "                       optimizer=tfk.optimizers.Adam\n",
    "                       )\n",
    "\n",
    "    print(\n",
    "        f'Generated a model with {model.count_params()} trainable parameters')\n",
    "\n",
    "    # checkpointsthe model to reload the best parameters\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree('./model_checkpoints')\n",
    "        print('Old checkpoint remove')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    model_save = tfk.callbacks.ModelCheckpoint('model_checkpoints')\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = tfk.callbacks.EarlyStopping('val_root_mean_squared_error',\n",
    "                                                 patience=10,\n",
    "                                                 restore_best_weights=True)\n",
    "    history = model.fit(input_dict,\n",
    "                        y_train.values,\n",
    "                        validation_data=(input_dict_test, y_test.values),\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=100,\n",
    "                        shuffle=True,\n",
    "                        # importance of the samples based of the ages\n",
    "                        sample_weight=weights_train.values,\n",
    "                        callbacks=[model_save, early_stopping],\n",
    "                        verbose=1,\n",
    "                        )\n",
    "    # return the validation accuracy for the last epoch.\n",
    "    rmse = history.history['val_root_mean_squared_error'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"RMSE: {:.2}\".format(rmse))\n",
    "    print()\n",
    "\n",
    "    global best_rmse\n",
    "    if rmse < best_rmse:\n",
    "        model.save('keras_best_model.h5')\n",
    "        best_rmse = rmse\n",
    "    print(\"Best RMSE: {:.2}\".format(rmse))\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory(not sure if needed in tf2)\n",
    "    del model, early_stopping, model_save\n",
    "    # Clear the Keras session\n",
    "    K.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    gc.collect()\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the optimization interuptible\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "checkpoint_callback = skopt.callbacks.CheckpointSaver(\"./result.pkl\")\n",
    "\n",
    "best_rmse = 1000\n",
    "gp_result = gp_minimize(func=fitness,\n",
    "                        dimensions=dimensions,\n",
    "                        n_calls=20,\n",
    "                        n_jobs=1,\n",
    "                        verbose=True,\n",
    "                        callback=[checkpoint_callback],\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gp_result.best_estimator_)\n",
    "print(gp_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_objective\n",
    "_ = plot_convergence(gp_result)\n",
    "_ = plot_objective(gp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the forecast results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "\n",
    "# prior distribution on the weights \n",
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t, scale=1),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "# layer posterior distribution with mean field approximation  \n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1.))\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t[..., :n],\n",
    "                       scale=1e-5 + 0.02*tf.nn.softplus(c + t[..., n:])),\n",
    "            reinterpreted_batch_ndims=1)),\n",
    "    ])\n",
    "\n",
    "# probabilistic  loss functions\n",
    "def neg_log_likelihood_continuous(y_true, y_pred):\n",
    "    '''\n",
    "    negative log likelyhood for stricly positive distributions \n",
    "    \n",
    "    '''\n",
    "    return -y_pred.prob(y_true+1e-6)\n",
    "\n",
    "\n",
    "def neg_log_likelihood_discrete(y_true, y_pred):\n",
    "    '''\n",
    "    negloglik when we don't care \n",
    "    '''\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "# custom metrics\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    '''\n",
    "    compact implemtation of the rmse \n",
    "    '''\n",
    "    return tf.math.sqrt(tf.math.reduce_mean(tf.math.square(y_pred-y_true)))\n",
    "\n",
    "\n",
    "# to much scaling: high variance, no enough -> converge to the likelyhood\n",
    "kl_weight = batch_size/training_size \n",
    "kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda\n",
    "                          kl_weight)  # KL over the batch of 2048 (should normalize by the number of mini batch)\n",
    "\n",
    "\n",
    "def create_mlp(layers_list=[512, 512, 512, 64],\n",
    "               type_output='poisson'):\n",
    "    '''\n",
    "    description : \n",
    "    generate regression mlp with\n",
    "    both embedding entries for categorical features and \n",
    "    standard inputs for numerical features\n",
    "\n",
    "    params:\n",
    "    layers_list : list of layers dimensions \n",
    "    output :\n",
    "    compiled keras model  \n",
    "    '''\n",
    "\n",
    "    # define our MLP network\n",
    "    layers = []\n",
    "    output_num = []\n",
    "    inputs = []\n",
    "    output_cat = []\n",
    "    output_num = []\n",
    "\n",
    "    # numerical data part\n",
    "    if len(num_feats) > 1:\n",
    "        for num_var in num_feats:\n",
    "            input_num = tfkl.Input(\n",
    "                shape=(1,), name='input_{0}'.format(num_var))\n",
    "            inputs.append(input_num)\n",
    "            output_num.append(input_num)\n",
    "        output_num = tfkl.Concatenate(name='concatenate_num')(output_num)\n",
    "        #output_num = tfkl.BatchNormalization()(output_num)\n",
    "\n",
    "    else:\n",
    "        input_num = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(numeric_features[0]))\n",
    "        inputs.append(input_num)\n",
    "        output_num = input_num\n",
    "\n",
    "    # create an embedding for every categorical feature\n",
    "    for categorical_var in cat_feats:\n",
    "        # should me nunique() but events are poorly preprocessed\n",
    "        no_of_unique_cat = cardinality[categorical_var]\n",
    "        print(categorical_var, no_of_unique_cat)\n",
    "        embedding_size = min(np.ceil((no_of_unique_cat)/2), 30)\n",
    "        embedding_size = int(embedding_size)\n",
    "        vocab = no_of_unique_cat+1\n",
    "        # functionnal loop\n",
    "        input_cat = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(categorical_var))\n",
    "        inputs.append(input_cat)\n",
    "        embedding = tfkl.Embedding(vocab,\n",
    "                                   embedding_size,\n",
    "                                   #embeddings_regularizer = tf.keras.regularizers.l1(1e-8),\n",
    "                                   name='embedding_{0}'.format(categorical_var))(input_cat)\n",
    "        embedding = tfkl.Dropout(0.1)(embedding)\n",
    "        vec = tfkl.Flatten(name='flatten_{0}'.format(\n",
    "            categorical_var))(embedding)\n",
    "        output_cat.append(vec)\n",
    "    output_cat = tfkl.Concatenate(name='concatenate_cat')(output_cat)\n",
    "\n",
    "    # concatenate numerical input and embedding output\n",
    "    dense = tfkl.Concatenate(name='concatenate_all')([output_num, output_cat])\n",
    "\n",
    "    for i in range(len(layers_list)):\n",
    "        dense = tfkl.Dense(layers_list[i],\n",
    "                           name='Dense_{0}'.format(str(i)),\n",
    "                           activation='elu')(dense)\n",
    "        dense = tfkl.Dropout(.1)(dense)\n",
    "        dense = tfkl.BatchNormalization()(dense)\n",
    "\n",
    "    # lognormal\n",
    "    if type_output == 'gaussian':\n",
    "        dense2 = tfk.layers.Dense(2,\n",
    "                                  activation='softplus',\n",
    "                                  name='Output'\n",
    "                                  )(dense)\n",
    "\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Normal(t[..., 0], scale=t[..., 1:]))(dense2)\n",
    "\n",
    "    # Poisson\n",
    "    elif type_output == 'poisson':\n",
    "        dense2 = tfk.layers.Dense(1,\n",
    "                                  name='Output')(dense)\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Poisson(rate=tf.math.softplus(t[..., 0])))(dense2)\n",
    "\n",
    "    # Gamma\n",
    "    elif type_output == 'gamma':\n",
    "        dense2 = tfk.layers.Dense(2,\n",
    "                                  name='Output',\n",
    "                                  activation='softplus',\n",
    "                                  )(dense)\n",
    "\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Gamma(concentration=.01*t[..., 0],\n",
    "                                rate=.01*t[..., 1]))(dense2)\n",
    "    else:\n",
    "        output = tfk.Dense(1)\n",
    "\n",
    "    model = tfk.Model(inputs, output)\n",
    "    opt = tfk.optimizers.Nadam(learning_rate=1e-2)\n",
    "\n",
    "    model.compile(loss=neg_log_likelihood_continuous,\n",
    "                  optimizer=opt,\n",
    "                  metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "\n",
    "try:\n",
    "    del mdl_tfp\n",
    "    print('model deleted ')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "mdl_tfp = create_mlp(type_output='poisson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "history = mdl_tfp.fit(input_dict,\n",
    "                      y_train.values,\n",
    "                      validation_data=(input_dict_test, y_test.values),\n",
    "                      batch_size=4096,\n",
    "                      epochs=2,\n",
    "                      shuffle=True,\n",
    "                      verbose=1,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make  a custom batch generator \n",
    "based on https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tfk.utils.Sequence):\n",
    "    'Generates data for tfk'\n",
    "\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32, 32, 32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # X : (n_samples, *dim, n_channels)\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i, ] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a custom random sampled training loop\n",
    "# # need to call the model with training = True for the custom training to handle dropout and BN\n",
    "\n",
    "# def random_batch(input_X,y,batch_size=128,sample_weights=None):\n",
    "#     # take a sample batch based on the weighting\n",
    "#     idx = np.random.choice(range(len(input_X)),sample_weights,batch_size)\n",
    "#     return input_X[idx],y[idx]\n",
    "\n",
    "\n",
    "# model = create_mlp()\n",
    "\n",
    "# # initialoze parameters\n",
    "# n_epochs = 5\n",
    "# batch_size = 2048\n",
    "# n_steps = 50\n",
    "# opt = tfk.optimizers.Adam(learning_rate=1e-3)\n",
    "# loss_fn = poisson\n",
    "# mean_loss = tfk.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fully bayesian network\n",
    "\n",
    "def create_mlp_full_bayes(layers_list=[512, 256, 128, 64],\n",
    "                          type_output='poisson'):\n",
    "    '''\n",
    "    description : \n",
    "    generate regression mlp with\n",
    "    both embedding entries for categorical features and \n",
    "    standard inputs for numerical features\n",
    "\n",
    "    params:\n",
    "    layers_list : list of layers dimensions \n",
    "    output :\n",
    "    compiled keras model  \n",
    "    '''\n",
    "\n",
    "    # define our MLP network\n",
    "    layers = []\n",
    "    output_num = []\n",
    "    inputs = []\n",
    "    output_cat = []\n",
    "    output_num = []\n",
    "\n",
    "    # numerical data part\n",
    "    if len(num_feats) > 1:\n",
    "        for num_var in num_feats:\n",
    "            input_num = tfkl.Input(\n",
    "                shape=(1,), name='input_{0}'.format(num_var))\n",
    "            inputs.append(input_num)\n",
    "            output_num.append(input_num)\n",
    "        output_num = tfkl.Concatenate(name='concatenate_num')(output_num)\n",
    "        #output_num = tfkl.BatchNormalization()(output_num)\n",
    "\n",
    "    else:\n",
    "        input_num = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(numeric_features[0]))\n",
    "        inputs.append(input_num)\n",
    "        output_num = input_num\n",
    "\n",
    "    # create an embedding for every categorical feature\n",
    "    for categorical_var in cat_feats:\n",
    "        # should me nunique() but events are poorly preprocessed\n",
    "        no_of_unique_cat = cardinality[categorical_var]\n",
    "        print(categorical_var, no_of_unique_cat)\n",
    "        embedding_size = 10  # min(np.ceil((no_of_unique_cat)/2), 10)\n",
    "        embedding_size = int(embedding_size)\n",
    "        vocab = no_of_unique_cat+1\n",
    "        # functionnal loop\n",
    "        input_cat = tfkl.Input(\n",
    "            shape=(1,), name='input_{0}'.format(categorical_var))\n",
    "        inputs.append(input_cat)\n",
    "        embedding = tfkl.Embedding(\n",
    "            vocab, embedding_size, name='embedding_{0}'.format(\n",
    "                categorical_var),\n",
    "            embeddings_initializer='random_uniform'\n",
    "        )(input_cat)\n",
    "        vec = tfkl.Flatten(name='flatten_{0}'.format(\n",
    "            categorical_var))(embedding)\n",
    "\n",
    "        output_cat.append(vec)\n",
    "    output_cat = tfkl.Concatenate(name='concatenate_cat')(output_cat)\n",
    "\n",
    "    # concatenate numerical input and embedding output\n",
    "    dense = tfkl.Concatenate(name='concatenate_all')([output_num, output_cat])\n",
    "\n",
    "    for i in range(len(layers_list)):\n",
    "        dense = tfp.layers.DenseVariational(layers_list[i],\n",
    "                                            activation='elu',\n",
    "                                            name='Dense_{0}'.format(str(i)),\n",
    "                                            make_posterior_fn=posterior_mean_field,\n",
    "                                            make_prior_fn=prior_trainable,\n",
    "                                            kl_weight=1/2048,\n",
    "                                            )(dense)\n",
    "        dense = tfp.bijectors.BatchNormalization()(dense)\n",
    "\n",
    "    if type_output == 'gaussian':\n",
    "        dense2 = tfp.layers.DenseVariational(2,\n",
    "                                             #activation = 'softplus',\n",
    "                                             make_posterior_fn=posterior_mean_field,\n",
    "                                             make_prior_fn=prior_trainable,\n",
    "                                             kl_weight=1/2048,\n",
    "                                             )(dense)\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.LogNormal(tf.math.softplus(0.05 * t[..., 0]),\n",
    "                                    scale=1e-6 + tf.math.softplus(0.05 * t[..., 1:])))(dense2)\n",
    "    # Poisson\n",
    "    elif type_output == 'poisson':\n",
    "        dense2 = tfp.layers.DenseVariational(2,\n",
    "                                             posterior_mean_field,\n",
    "                                             prior_trainable,\n",
    "                                             # kl_weight=1/100000\n",
    "                                             )(dense)\n",
    "        dense2 = tfp.layers.DenseFlipout(2)(dense)\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Poisson(rate=tf.math.softplus(t[..., 0])))(dense2)\n",
    "\n",
    "    # Gamma\n",
    "    elif type_output == 'gamma':\n",
    "        #         dense2 = tfp.layers.DenseFlipout(2,\n",
    "        #                                   name='Output',\n",
    "        #                                   activation = 'softplus',\n",
    "        # #                                   kernel_prior =prior_trainable,\n",
    "        # #                                  kernel_divergence_fn=kl_divergence_function,\n",
    "        #                                  )(dense)\n",
    "        dense2 = tfp.layers.DenseVariational(2,\n",
    "                                             name='Output',\n",
    "                                             activation='linear',\n",
    "                                             make_posterior_fn=posterior_mean_field,\n",
    "                                             kl_weight=1/2048,\n",
    "\n",
    "                                             make_prior_fn=prior_trainable,\n",
    "                                             )(dense)\n",
    "        output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Gamma(concentration=t[..., 0],\n",
    "                                rate=t[..., 1]))(dense2)\n",
    "\n",
    "    else:\n",
    "        output = tfk.Dense(1, kernel_divergence_fn=kl_divergence_function,\n",
    "                           name='Output')(dense)\n",
    "\n",
    "    model = tfk.Model(inputs, output)\n",
    "    opt = tfk.optimizers.Adam(learning_rate=1e-3)\n",
    "    # kl divergence is direclty added to the loss function\n",
    "    model.compile(loss=neg_log_likelihood_continuous, optimizer=opt,\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
