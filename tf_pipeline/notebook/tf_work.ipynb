{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, random,datetime\n",
    "\n",
    "from tf_utils import *\n",
    "sys.path.append(\".\") # For execution form the main file \n",
    "sys.path.append(\"..\") # For execution from the notebook \n",
    "\n",
    "# custom imports\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL_DTYPES={\"event_name_1\": \"category\",\n",
    "            \"event_name_2\": \"category\",\n",
    "            \"event_type_1\": \"category\", \n",
    "            \"event_type_2\": \"category\",\n",
    "            \"weekday\": \"category\", \n",
    "            'wm_yr_wk': 'int16', \"wday\": \"int16\",\n",
    "            \"month\": \"int16\", \"year\": \"int16\",\n",
    "            \"snap_CA\": \"float32\",\n",
    "            'snap_TX': 'float32',\n",
    "            'snap_WI': 'float32' }\n",
    "PRICE_DTYPES = {\"store_id\": \"category\",\n",
    "                \"item_id\": \"category\",\n",
    "                \"wm_yr_wk\": \"int16\", \n",
    "                \"sell_price\":\"float32\" }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 28 \n",
    "max_lags = 57\n",
    "tr_last = 1913\n",
    "fday = datetime.datetime(2016,4, 25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dt(is_train = True, nrows = None, first_day = 1200):\n",
    "    prices = pd.read_csv(\"../data/raw/sell_prices.csv\", dtype = PRICE_DTYPES)\n",
    "    for col, col_dtype in PRICE_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            prices[col] = prices[col].cat.codes.astype(\"int16\")\n",
    "            prices[col] -= prices[col].min()\n",
    "            \n",
    "    cal = pd.read_csv(\"../data/raw/calendar.csv\", dtype = CAL_DTYPES)\n",
    "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
    "    for col, col_dtype in CAL_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            cal[col] = cal[col].cat.codes.astype(\"int16\")\n",
    "            cal[col] -= cal[col].min()\n",
    "    \n",
    "    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n",
    "    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n",
    "    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "    dtype = {numcol:\"float32\" for numcol in numcols} \n",
    "    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n",
    "    dt = pd.read_csv(\"../data/raw/sales_train_validation.csv\", \n",
    "                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n",
    "    \n",
    "    for col in catcols:\n",
    "        if col != \"id\":\n",
    "            dt[col] = dt[col].cat.codes.astype(\"int16\")\n",
    "            dt[col] -= dt[col].min()\n",
    "    \n",
    "    if not is_train:\n",
    "        for day in range(tr_last+1, tr_last+ 28 +1):\n",
    "            dt[f\"d_{day}\"] = np.nan\n",
    "    \n",
    "    dt = pd.melt(dt,\n",
    "                  id_vars = catcols,\n",
    "                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n",
    "                  var_name = \"d\",\n",
    "                  value_name = \"sales\")\n",
    "    \n",
    "    dt = dt.merge(cal, on= \"d\", copy = False)\n",
    "    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "    \n",
    "    return dt\n",
    "\n",
    "def create_fea(dt):\n",
    "    lags = [7, 28]\n",
    "    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
    "\n",
    "    wins = [7, 28]\n",
    "    for win in wins :\n",
    "        for lag,lag_col in zip(lags, lag_cols):\n",
    "            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n",
    "\n",
    "    \n",
    "    \n",
    "    date_features = {\n",
    "        \n",
    "        \"wday\": \"weekday\",\n",
    "        \"week\": \"weekofyear\",\n",
    "        \"month\": \"month\",\n",
    "        \"quarter\": \"quarter\",\n",
    "        \"year\": \"year\",\n",
    "        \"mday\": \"day\",\n",
    "    }\n",
    "    \n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        if date_feat_name in dt.columns:\n",
    "            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n",
    "        else:\n",
    "            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMORY USAGE : 5.06307527\n"
     ]
    }
   ],
   "source": [
    "FIRST_DAY = 1 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !\n",
    "df = create_dt(is_train=True, first_day= FIRST_DAY)\n",
    "create_fea(df)\n",
    "print(f\"MEMORY USAGE : {df.memory_usage().sum()/1e9}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = ['item_id', \n",
    "             'dept_id',\n",
    "             'store_id',\n",
    "             'cat_id',\n",
    "             'state_id'] +\\\n",
    "            [\"event_name_1\",\n",
    "             \"event_name_2\",\n",
    "             \"event_type_1\",\n",
    "             \"event_type_2\"]\n",
    "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage().sum()/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weights for the training (the older the sample the less it will have impact )\n",
    "weights = df['d'].str[2:].astype(int)\n",
    "weights = weights/np.max(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_feats = df.columns[~df.columns.isin(useless_cols+cat_feats)].to_list()\n",
    "train_cols = num_feats+cat_feats\n",
    "\n",
    "X_train = df[train_cols]\n",
    "y_train = df[\"sales\"]\n",
    "\n",
    "# np.random.seed(777)\n",
    "# fake_valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\n",
    "# train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n",
    "\n",
    "# X_test,y_test = X_train.loc[fake_valid_inds],y_train.loc[fake_valid_inds]\n",
    "# X_train,y_train = X_train.loc[train_inds],y_train.loc[train_inds]\n",
    "cardinality  = df[cat_feats].max()\n",
    "weights_train =  weights.loc[X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further preprocessing \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_train[num_feats] = scaler.fit_transform(X_train[num_feats])\n",
    "# X_test[num_feats] = scaler.fit_transform(X_test[num_feats])\n",
    "\n",
    "# prepare input for tensorflow \n",
    "# as we have multiple input type the best solution is to feed a dict like object \n",
    "\n",
    "input_dict = {f\"input_{col}\": X_train[col] for col in X_train.columns}\n",
    "# input_dict_test = {f\"input_{col}\": X_test[col] for col in X_train.columns}\n",
    "\n",
    "del df,X_train,X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF2 model \n",
    "\n",
    "# Dense model, not sequential\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "tfkl = tfk.layers\n",
    "\n",
    "# # loss for a poisson regression \n",
    "# def poisson(y_true, y_pred): \n",
    "#     return K.mean(K.maximum(.0, y_pred) - y_true * K.log(K.maximum(.0, y_pred) + K.epsilon()), axis=-1)\n",
    "\n",
    "# def tweedie_loss(y_true, y_pred):\n",
    "#     p=1.5\n",
    "#     dev = K.pow(y_true, 2-p)/((1-p) * (2-p)) \\\n",
    "#     - y_true * K.pow(K.maximum(.0, y_pred)+ K.epsilon(), 1-p)/(1-p) \\\n",
    "#     + K.pow(K.maximum(.0, y_pred)+ K.epsilon(), 2-p)/(2-p)\n",
    "#     return K.mean(dev,axis=-1)\n",
    "\n",
    "# alpha=.5\n",
    "# def weighted_loss(y_true, y_pred):\n",
    "#     ''' make a comprised loss of poisson and tweedie distribution'''\n",
    "#     return (1 - alpha) * poisson(y_true, y_pred) + alpha * tweedie(y_true, y_pred)\n",
    "\n",
    "# def create_mlp(layers_list=[512,256,128,64]):\n",
    "#     '''\n",
    "#     description : \n",
    "#     generate regression mlp with\n",
    "#     both embedding entries for categorical features and \n",
    "#     standard inputs for numerical features\n",
    "\n",
    "#     params:\n",
    "#     layers_list : list of layers dimensions \n",
    "#     output :\n",
    "#     compiled keras model  \n",
    "#     '''\n",
    "\n",
    "#     # define our MLP network\n",
    "#     layers = []\n",
    "#     output_num = []\n",
    "#     inputs = []\n",
    "#     output_cat = []\n",
    "#     output_num = []\n",
    "    \n",
    "#     # sequencial inputs \n",
    "\n",
    "#     # numerical data part\n",
    "#     if len(num_feats) > 1:\n",
    "#         for num_var in num_feats:\n",
    "#             print(num_var)\n",
    "#             input_num = tfkl.Input(\n",
    "#                 shape=(1,), name='input_{0}'.format(num_var))\n",
    "#             inputs.append(input_num)\n",
    "#             output_num.append(input_num)\n",
    "#         output_num = tfkl.Concatenate(name='concatenate_num')(output_num)\n",
    "#         output_num = tfkl.BatchNormalization()(output_num)\n",
    "\n",
    "#     else:\n",
    "#         input_num = tfkl.Input(\n",
    "#             shape=(1,), name='input_{0}'.format(numeric_features[0]))\n",
    "#         inputs.append(input_num)\n",
    "#         output_num = input_num\n",
    "\n",
    "#     # categorical data input \n",
    "#     for categorical_var in cat_feats:\n",
    "#         no_of_unique_cat = cardinality[categorical_var] # should me nunique() but events are poorly preprocessed \n",
    "#         print(categorical_var , no_of_unique_cat)\n",
    "#         embedding_size = min(np.ceil((no_of_unique_cat)/2), 50)\n",
    "#         embedding_size = int(embedding_size)\n",
    "#         vocab = no_of_unique_cat+1\n",
    "#         # functionnal loop\n",
    "#         input_cat = tfkl.Input(\n",
    "#             shape=(1,), name='input_{0}'.format(categorical_var))\n",
    "#         inputs.append(input_cat)\n",
    "#         embedding = tfkl.Embedding(vocab,\n",
    "#                                    embedding_size,\n",
    "#                                    embeddings_regularizer = tf.keras.regularizers.l1(1e-8),\n",
    "#                                    name='embedding_{0}'.format(categorical_var))(input_cat)\n",
    "#         embedding = tfkl.Dropout(0.1)(embedding)\n",
    "#         vec = tfkl.Flatten(name='flatten_{0}'.format(\n",
    "#             categorical_var))(embedding)\n",
    "        \n",
    "#         output_cat.append(vec)\n",
    "#     output_cat = tfkl.Concatenate(name='concatenate_cat')(output_cat)\n",
    "\n",
    "#     # concatenate numerical input and embedding output\n",
    "#     dense = tfkl.Concatenate(name='concatenate_all')([output_num, output_cat])\n",
    "\n",
    "#     for i in range(len(layers_list)):\n",
    "#         dense = tfkl.Dense(layers_list[i],\n",
    "#                            name='Dense_{0}'.format(str(i)),\n",
    "#                            activation='elu')(dense)\n",
    "#         dense = tfkl.Dropout(.1)(dense)\n",
    "#         dense = tfkl.BatchNormalization()(dense)\n",
    "\n",
    "#     dense2 = tfkl.Dense(1, name='Output', activation='elu')(dense)\n",
    "#     model = tfk.Model(inputs, dense2)\n",
    "\n",
    "#     opt = tfk.optimizers.Adam(learning_rate=1e-2)\n",
    "#     model.compile(loss=poisson, optimizer=opt, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "#     return model\n",
    "\n",
    "# try:\n",
    "#     del mdl\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "create_mlp(layers_list=[512,256,128,64],\n",
    "           emb_dim=50,\n",
    "           loss_fn='poisson',\n",
    "           learning_rate=1e-2,\n",
    "           optimizer=tfk.optimizers.Adam,\n",
    "           cat_feats=cat_feats,\n",
    "           num_feats=num_feats,\n",
    "           cardinality=cardinality, verbose=1)\n",
    "mdl = create_mlp()\n",
    "\n",
    "# mdl = create_mlp()\n",
    "# mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints  \n",
    "model_save = tfk.callbacks.ModelCheckpoint('model_checkpoints')\n",
    "early_stopping = tfk.callbacks.EarlyStopping('val_root_mean_squared_error',\n",
    "                                             patience = 7,\n",
    "                                            restore_best_weights=True)\n",
    "history = mdl.fit(input_dict,\n",
    "                  y_train.values,\n",
    "                  #validation_data=(input_dict_test, y_test.values),\n",
    "                  batch_size=4096,\n",
    "                  epochs=100,\n",
    "                  shuffle=True,\n",
    "                  sample_weight = weights_train.values,\n",
    "                  callbacks=[early_stopping],\n",
    "                  verbose=1,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.save('keras_poisson_stable2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt \n",
    "# plt.plot(history.history['loss'])\n",
    "# # plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'valid'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(history.history['root_mean_squared_error'])\n",
    "# # plt.plot(history.history['val_loss'])\n",
    "# plt.title('model rmse')\n",
    "# plt.ylabel('rmse')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'valid'], loc='upper left')\n",
    "# plt.show()\n",
    "# plt.savefig('poisson.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta \n",
    "alphas = [1.035, 1.03, 1.025]\n",
    "weights = [1/len(alphas)]*len(alphas)\n",
    "sub = 0.\n",
    "\n",
    "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
    "\n",
    "    te = create_dt(False)\n",
    "    cols = [f\"F{i}\" for i in range(1,29)]\n",
    "\n",
    "    for tdelta in range(0, 28):\n",
    "        day = fday + timedelta(days=tdelta)\n",
    "        print(icount, day)\n",
    "        tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n",
    "        create_fea(tst)\n",
    "        tst = tst.loc[tst.date == day , train_cols]\n",
    "        input_dict_predict = {f\"input_{col}\": tst[col] for col in tst.columns}\n",
    "        pred = mdl.predict(input_dict_predict,batch_size=10000)\n",
    "        te.loc[te.date == day, \"sales\"] = alpha*pred\n",
    "        print(pred)\n",
    "\n",
    "\n",
    "    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
    "#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n",
    "#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n",
    "    te_sub.fillna(0., inplace = True)\n",
    "    te_sub.sort_values(\"id\", inplace = True)\n",
    "    te_sub.reset_index(drop=True, inplace = True)\n",
    "    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n",
    "    if icount == 0 :\n",
    "        sub = te_sub\n",
    "        sub[cols] *= weight\n",
    "    else:\n",
    "        sub[cols] += te_sub[cols]*weight\n",
    "    print(icount, alpha, weight)\n",
    "\n",
    "sub2 = sub.copy()\n",
    "sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "sub.to_csv(\"submission_tf_stable.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # submit using the tensorflow model \n",
    "\n",
    "# alphas = [1.035, 1.03, 1.025]\n",
    "# weights = [1/len(alphas)]*len(alphas)\n",
    "# sub = 0.\n",
    "\n",
    "# for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
    "\n",
    "#     te = create_dt(False)\n",
    "#     cols = [f\"F{i}\" for i in range(1,29)]\n",
    "\n",
    "#     for tdelta in range(0, 28):\n",
    "#         day = fday + timedelta(days=tdelta)\n",
    "#         print(icount, day)\n",
    "#         tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n",
    "#         create_fea(tst)\n",
    "#         tst = tst.loc[tst.date == day , train_cols]\n",
    "#         input_dict_predict = {f\"input_{col}\": tst[col] for col in tst.columns}\n",
    "#         te.loc[te.date == day, \"sales\"] = alpha*mdl.predict(input_dict_predict,batch_size=10000) # magic multiplier by kyakovlev\n",
    "\n",
    "#     te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
    "#     te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "#     te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n",
    "#     te_sub.fillna(0., inplace = True)\n",
    "#     te_sub.sort_values(\"id\", inplace = True)\n",
    "#     te_sub.reset_index(drop=True, inplace = True)\n",
    "#     te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n",
    "#     if icount == 0 :\n",
    "#         sub = te_sub\n",
    "#         sub[cols] *= weight\n",
    "#     else:\n",
    "#         sub[cols] += te_sub[cols]*weight\n",
    "#     print(icount, alpha, weight)\n",
    "\n",
    "# sub2 = sub.copy()\n",
    "# sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "# sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "# sub.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
