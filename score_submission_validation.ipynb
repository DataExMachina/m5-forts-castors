{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluation metric\n",
    "## from https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834 and edited to get scores at all levels\n",
    "class WRMSSEEvaluator(object):\n",
    "\n",
    "    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n",
    "        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n",
    "        train_target_columns = train_y.columns.tolist()\n",
    "        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n",
    "\n",
    "        train_df['all_id'] = 0  # for lv1 aggregation\n",
    "\n",
    "        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n",
    "        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n",
    "\n",
    "        if not all([c in valid_df.columns for c in id_columns]):\n",
    "            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "\n",
    "        self.weight_columns = weight_columns\n",
    "        self.id_columns = id_columns\n",
    "        self.valid_target_columns = valid_target_columns\n",
    "\n",
    "        weight_df = self.get_weight_df()\n",
    "\n",
    "        self.group_ids = (\n",
    "            'all_id',\n",
    "            'cat_id',\n",
    "            'state_id',\n",
    "            'dept_id',\n",
    "            'store_id',\n",
    "            'item_id',\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "        )\n",
    "\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids)):\n",
    "            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n",
    "            scale = []\n",
    "            for _, row in train_y.iterrows():\n",
    "                series = row.values[np.argmax(row.values != 0):]\n",
    "                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n",
    "            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n",
    "            setattr(self, f'lv{i + 1}_train_df', train_y)\n",
    "            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n",
    "\n",
    "            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n",
    "            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n",
    "\n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
    "        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n",
    "        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n",
    "        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n",
    "\n",
    "        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n",
    "        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n",
    "        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n",
    "        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n",
    "        return weight_df\n",
    "\n",
    "    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n",
    "        valid_y = getattr(self, f'lv{lv}_valid_df')\n",
    "        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n",
    "        scale = getattr(self, f'lv{lv}_scale')\n",
    "        return (score / scale).map(np.sqrt)\n",
    "\n",
    "    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]):\n",
    "        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
    "\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
    "\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n",
    "\n",
    "        group_ids = []\n",
    "        all_scores = []\n",
    "        for i, group_id in enumerate(self.group_ids):\n",
    "            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n",
    "            weight = getattr(self, f'lv{i + 1}_weight')\n",
    "            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n",
    "            group_ids.append(group_id)\n",
    "            all_scores.append(lv_scores.sum())\n",
    "\n",
    "        return group_ids, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d318d5536c84a618f0a858046c93fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data & init evaluator\n",
    "df_train_full = pd.read_csv(\"data/raw/sales_train_evaluation.csv\")\n",
    "df_calendar = pd.read_csv(\"data/raw/calendar.csv\")\n",
    "df_prices = pd.read_csv(\"data/raw/sell_prices.csv\")\n",
    "df_sample_submission = pd.read_csv(\"data/raw/sample_submission.csv\")\n",
    "df_sample_submission[\"order\"] = range(df_sample_submission.shape[0])\n",
    "\n",
    "df_train = df_train_full.iloc[:, :-28]\n",
    "df_valid = df_train_full.iloc[:, -28:]\n",
    "\n",
    "evaluator = WRMSSEEvaluator(df_train, df_valid, df_calendar, df_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/submission/tf_estim_validation.csv',\n",
       " 'data/submission/Prophet_store_dpt_tf_weights_validation.csv',\n",
       " 'data/submission/Prophet_store_dpt_lgb_weights_validation.csv',\n",
       " 'data/submission/lgb_estim_validation.csv',\n",
       " 'data/submission/ens_estim_validation.csv',\n",
       " 'data/submission/Wavenet_store_dpt_lgb_weights_validation.csv',\n",
       " 'data/submission/lgb_tf_prophet_ensembling_validation.csv',\n",
       " 'data/submission/Wavenet_store_dpt_tf_weights_validation.csv',\n",
       " 'data/submission/Lgb_store_dpt_tf_weights_validation.csv',\n",
       " 'data/submission/Lgb_store_dpt_lgb_weights_validation.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = glob.glob(\"data/submission/*_validation.csv\")\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_submission_path = 'data/submission/Prophet_store_dpt_ens_weights_validation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for group all_id: 0.40981\n",
      "Score for group cat_id: 0.42981\n",
      "Score for group state_id: 0.46337\n",
      "Score for group dept_id: 0.52668\n",
      "Score for group store_id: 0.51798\n",
      "Score for group item_id: 0.83595\n",
      "Score for group ['state_id', 'cat_id']: 0.50182\n",
      "Score for group ['state_id', 'dept_id']: 0.58756\n",
      "Score for group ['store_id', 'cat_id']: 0.5667\n",
      "Score for group ['store_id', 'dept_id']: 0.65686\n",
      "Score for group ['item_id', 'state_id']: 0.83619\n",
      "Score for group ['item_id', 'store_id']: 0.83168\n",
      "\n",
      "Public LB Score: 0.59703\n"
     ]
    }
   ],
   "source": [
    "## evaluating your submission\n",
    "preds_valid = pd.read_csv(your_submission_path)\n",
    "preds_valid = preds_valid[preds_valid.id.str.contains(\"validation\")]\n",
    "preds_valid = preds_valid.merge(df_sample_submission[[\"id\", \"order\"]], on = \"id\").sort_values(\"order\").drop([\"id\", \"order\"], axis = 1).reset_index(drop = True)\n",
    "preds_valid.rename(columns = {\n",
    "    \"F1\": \"d_1914\", \"F2\": \"d_1915\", \"F3\": \"d_1916\", \"F4\": \"d_1917\", \"F5\": \"d_1918\", \"F6\": \"d_1919\", \"F7\": \"d_1920\",\n",
    "    \"F8\": \"d_1921\", \"F9\": \"d_1922\", \"F10\": \"d_1923\", \"F11\": \"d_1924\", \"F12\": \"d_1925\", \"F13\": \"d_1926\", \"F14\": \"d_1927\",\n",
    "    \"F15\": \"d_1928\", \"F16\": \"d_1929\", \"F17\": \"d_1930\", \"F18\": \"d_1931\", \"F19\": \"d_1932\", \"F20\": \"d_1933\", \"F21\": \"d_1934\",\n",
    "    \"F22\": \"d_1935\", \"F23\": \"d_1936\", \"F24\": \"d_1937\", \"F25\": \"d_1938\", \"F26\": \"d_1939\", \"F27\": \"d_1940\", \"F28\": \"d_1941\"\n",
    "}, inplace = True)\n",
    "\n",
    "groups, scores = evaluator.score(preds_valid)\n",
    "\n",
    "score_public_lb = np.mean(scores)\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    print(f\"Score for group {groups[i]}: {round(scores[i], 5)}\")\n",
    "\n",
    "print(f\"\\nPublic LB Score: {round(score_public_lb, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation scores:\n",
    "\n",
    "- Lgb_store_dpt_lgb_weights_validation : 0.6598\n",
    "- Wavenet_store_dpt_lgb_weights_validation : 0.62519\n",
    "- Prophet_store_dpt_lgb_weights_validation: 0.59705\n",
    "- tf_estim_validation : 0.52697\n",
    "- lgb_estim_validation : 0.47081\n",
    "\n",
    "\n",
    "- ens_estim_validation (recurrent) : 0.50853\n",
    "- lgb_tf_prophet_ensembling : 0.47369"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
